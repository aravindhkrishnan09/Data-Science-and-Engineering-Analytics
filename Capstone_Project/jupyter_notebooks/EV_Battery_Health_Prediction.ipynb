{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1d36624",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.io import whosmat\n",
    "from scipy.io import loadmat\n",
    "from io import BytesIO\n",
    "import datetime\n",
    "\n",
    "print(sys.executable)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cf4d00f",
   "metadata": {},
   "source": [
    "### Dataset Acquisition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9028d6e9-fbd1-40a0-acc6-6196543f5c87",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(whosmat(r'G:\\DIYguru\\Data-Science-and-Engineering-Analytics\\Capstone_Project\\source_data\\B0005.mat'))\n",
    "print(whosmat(r'G:\\DIYguru\\Data-Science-and-Engineering-Analytics\\Capstone_Project\\source_data\\B0006.mat'))\n",
    "print(whosmat(r'G:\\DIYguru\\Data-Science-and-Engineering-Analytics\\Capstone_Project\\source_data\\B0007.mat'))\n",
    "print(whosmat(r'G:\\DIYguru\\Data-Science-and-Engineering-Analytics\\Capstone_Project\\source_data\\B0018.mat'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c65813d-171e-4ae1-9627-3471e112d0aa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "file_path_B0005 = r'G:\\DIYguru\\Data-Science-and-Engineering-Analytics\\Capstone_Project\\source_data\\B0005.mat'\n",
    "file_path_B0006 = r'G:\\DIYguru\\Data-Science-and-Engineering-Analytics\\Capstone_Project\\source_data\\B0006.mat'\n",
    "file_path_B0007 = r'G:\\DIYguru\\Data-Science-and-Engineering-Analytics\\Capstone_Project\\source_data\\B0007.mat'\n",
    "file_path_B0018 = r'G:\\DIYguru\\Data-Science-and-Engineering-Analytics\\Capstone_Project\\source_data\\B0018.mat'\n",
    "\n",
    "data_B0005 = loadmat(file_path_B0005, struct_as_record=False, squeeze_me=True)\n",
    "data_B0006 = loadmat(file_path_B0006, struct_as_record=False, squeeze_me=True)\n",
    "data_B0007 = loadmat(file_path_B0007, struct_as_record=False, squeeze_me=True)\n",
    "data_B0018 = loadmat(file_path_B0018, struct_as_record=False, squeeze_me=True)\n",
    "\n",
    "# Access the struct\n",
    "b0005 = data_B0005['B0005']\n",
    "b0006 = data_B0006['B0006']\n",
    "b0007 = data_B0007['B0007']\n",
    "b0018 = data_B0018['B0018']\n",
    "\n",
    "# Check what attributes (fields) this struct has\n",
    "print('b0005 \\n',dir(b0005))\n",
    "print('b0006 \\n',dir(b0006))\n",
    "print('b0007 \\n',dir(b0007))\n",
    "print('b0018 \\n',dir(b0018))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aad61514",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(b0005.cycle)\n",
    "print(b0006.cycle)\n",
    "print(b0007.cycle)\n",
    "print(b0018.cycle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4933126e-891b-46e6-89ce-9731e0f171c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "cycles = b0005.cycle\n",
    "#cycles = b0006.cycle\n",
    "#cycles = b0007.cycle\n",
    "#cycles = b0018.cycle\n",
    "\n",
    "print(type(cycles))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b83c0713",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(cycles),\"\\n\")\n",
    "\n",
    "print(cycles[0],\"\\n\")\n",
    "print([col for col in dir(cycles[0]) if not col.startswith('_')],\"\\n\") \n",
    "\n",
    "print(cycles[0].data,\"\\n\")\n",
    "print([col for col in dir(cycles[0].data) if not col.startswith('_')],\"\\n\") \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3c935b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(cycles)):\n",
    "    print(\"cycle\",i,\": \", cycles[i],\"--\",[col for col in dir(cycles[i]) if not col.startswith('_')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b388a2b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(cycles)):\n",
    "    print(\"cycle\",i,\": \", cycles[i],\"--\",[col for col in dir(cycles[i].data) if not col.startswith('_')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8295e11e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(cycles)):\n",
    "    print(f\"cycle {i}: ambient_temperature = {cycles[i].ambient_temperature}\")\n",
    "    print(f\"cycle {i}: time = {cycles[i].time}\")\n",
    "    print(f\"cycle {i}: type = {cycles[i].type}\")\n",
    "    if cycles[i].type == 'charge':\n",
    "        print(f\"cycle {i}: Current_charge = {cycles[i].data.Current_charge}\")\n",
    "        print(f\"cycle {i}: Current_measured = {cycles[i].data.Current_measured}\")\n",
    "        print(f\"cycle {i}: Temperature_measured = {cycles[i].data.Temperature_measured}\")\n",
    "        print(f\"cycle {i}: Time = {cycles[i].data.Time}\")\n",
    "        print(f\"cycle {i}: Voltage_charge = {cycles[i].data.Voltage_charge}\")\n",
    "        print(f\"cycle {i}: Voltage_measured = {cycles[i].data.Voltage_measured}\")\n",
    "    elif cycles[i].type == 'discharge':\n",
    "        print(f\"cycle {i}: Capacity = {cycles[i].data.Capacity}\")\n",
    "        print(f\"cycle {i}: Current_load = {cycles[i].data.Current_load}\")\n",
    "        print(f\"cycle {i}: Current_measured = {cycles[i].data.Current_measured}\")\n",
    "        print(f\"cycle {i}: Temperature_measured = {cycles[i].data.Temperature_measured}\")\n",
    "        print(f\"cycle {i}: Time = {cycles[i].data.Time}\")\n",
    "        print(f\"cycle {i}: Voltage_load = {cycles[i].data.Voltage_load}\")\n",
    "        print(f\"cycle {i}: Voltage_measured = {cycles[i].data.Voltage_measured}\")\n",
    "    elif cycles[i].type == 'impedance':\n",
    "        print(f\"cycle {i}: Battery_current = {cycles[i].data.Battery_current}\")\n",
    "        print(f\"cycle {i}: Battery_impedance = {cycles[i].data.Battery_impedance}\")\n",
    "        print(f\"cycle {i}: Current_ratio = {cycles[i].data.Current_ratio}\")\n",
    "        print(f\"cycle {i}: Rct = {cycles[i].data.Rct}\")\n",
    "        print(f\"cycle {i}: Re = {cycles[i].data.Re}\")\n",
    "        print(f\"cycle {i}: Rectified_Impedance = {cycles[i].data.Rectified_Impedance}\")\n",
    "        print(f\"cycle {i}: Sense_current = {cycles[i].data.Sense_current}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cbdf0bc",
   "metadata": {},
   "source": [
    "### Function Explanation\n",
    "\n",
    "- **def export_cycles_flattened(battery_name, cycles, export_filename, N=None):**\n",
    "    - Defines a function to flatten and export battery cycle data.\n",
    "    - `battery_name`: Name/identifier of the battery (used in output).\n",
    "    - `cycles`: Iterable/list of cycle objects (each could be charge/discharge/impedance).\n",
    "    - `export_filename`: Output filename for the CSV.\n",
    "    - `N`: Number of cycles to process (if `None` or `0`, process all cycles).\n",
    "\n",
    "- **rows = []**\n",
    "    - Initialize an empty list to store each \"flattened\" row.\n",
    "\n",
    "- **total_cycles = len(cycles) if N is None or N == 0 else min(N, len(cycles))**\n",
    "    - Decide how many cycles to process: all if `N` not provided, or up to `N` cycles.\n",
    "\n",
    "- **for i, cycle in enumerate(cycles):**\n",
    "    - Loop over each cycle with index `i`.\n",
    "\n",
    "- **if i >= total_cycles:**\n",
    "    - If processed enough cycles, break out of loop.\n",
    "\n",
    "- **cycle_type = getattr(cycle, 'type', None)**\n",
    "    - Get the cycle's type attribute ('charge', 'discharge', 'impedance', etc).\n",
    "\n",
    "- **if not hasattr(cycle, 'data'):**\n",
    "    - If the cycle has no 'data' attribute, skip this cycle.\n",
    "\n",
    "- **data = cycle.data**\n",
    "    - Get the data object for the current cycle.\n",
    "\n",
    "- **if cycle_type == 'charge':**\n",
    "    - For charge cycles, define relevant columns.\n",
    "\n",
    "- **columns = [...]**\n",
    "    - Specify the list of attributes to extract, depending on the cycle type.\n",
    "\n",
    "- **elif cycle_type == 'discharge':**\n",
    "    - For discharge cycles, define different relevant columns.\n",
    "\n",
    "- **elif cycle_type == 'impedance':**\n",
    "    - For impedance cycles, yet another set of columns is defined.\n",
    "\n",
    "- **else:**\n",
    "    - If cycle type doesn't match any expected, skip the cycle.\n",
    "\n",
    "- **arr_lens = []**\n",
    "    - Prepare to find the length of each data array for the chosen columns.\n",
    "\n",
    "- **for col in columns:**\n",
    "    - For each column to extract:\n",
    "\n",
    "- **arr = getattr(data, col, None)**\n",
    "    - Get the array for the current column, if it exists.\n",
    "\n",
    "- **if arr is not None and hasattr(arr, '__len__'):**\n",
    "    - Check if the data exists and is an array with a length.\n",
    "\n",
    "- **arr_lens.append(len(arr))**\n",
    "    - Store the array length for this column.\n",
    "\n",
    "- **if not arr_lens:**\n",
    "    - If no arrays found, skip this cycle.\n",
    "\n",
    "- **min_len = min(arr_lens)**\n",
    "    - Find the (minimum) length of these arrays; ensures no index-out-of-bounds below.\n",
    "\n",
    "- **for idx in range(min_len):**\n",
    "    - Loop over array indices, up to the shortest array length (to keep data aligned).\n",
    "\n",
    "- **row = { ... }**\n",
    "    - Create a new dictionary to represent a single \"flattened\" row, including:\n",
    "        - battery_name, cycle index, type, ambient temp, time (as a datetime).\n",
    "\n",
    "- **if cycle_type == 'impedance':**\n",
    "    - If current type is impedance, columns are postprocessed for real part.\n",
    "\n",
    "- **for col in columns:**\n",
    "    - For all columns of this cycle type:\n",
    "\n",
    "- **arr = getattr(data, col, None)**\n",
    "    - Fetch the column's array again.\n",
    "\n",
    "- **value = arr[idx] if arr is not None and hasattr(arr, '__getitem__') else None**\n",
    "    - Safely retrieve the value at current index.\n",
    "\n",
    "- **row[col] = np.real(value) if value is not None else None**\n",
    "    - For impedance, store just the real component if it's complex; else None.\n",
    "\n",
    "- **else:**\n",
    "    - For other types ('charge'/'discharge'):\n",
    "\n",
    "- **row[col] = value**\n",
    "    - Store the value as-is.\n",
    "\n",
    "- **rows.append(row)**\n",
    "    - Add this row (dictionary) to the list of all rows.\n",
    "\n",
    "- **df = pd.DataFrame(rows)**\n",
    "    - Create a pandas DataFrame from the collected rows.\n",
    "\n",
    "- **df.to_csv(export_filename, index=False)**\n",
    "    - Save the DataFrame to CSV (no row index).\n",
    "\n",
    "- **print(f\"Exported flattened DataFrame to {export_filename}\")**\n",
    "    - Print a confirmation message.\n",
    "\n",
    "- **return df**\n",
    "    - Return the created DataFrame for further use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebe0bdf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_cycles_flattened(battery_name, cycles, export_filename, N=None):\n",
    "\n",
    "    # No more type_counts logic, handle all cycles or as per N\n",
    "    rows = []\n",
    "    total_cycles = len(cycles) if N is None or N == 0 else min(N, len(cycles))\n",
    "\n",
    "    for i, cycle in enumerate(cycles):\n",
    "        if i >= total_cycles:\n",
    "            break\n",
    "        cycle_type = getattr(cycle, 'type', None)\n",
    "        if not hasattr(cycle, 'data'):\n",
    "            continue\n",
    "\n",
    "        data = cycle.data\n",
    "        if cycle_type == 'charge':\n",
    "            columns = ['Current_charge', 'Current_measured', 'Temperature_measured', 'Time', 'Voltage_charge', 'Voltage_measured']\n",
    "        elif cycle_type == 'discharge':\n",
    "            columns = ['Capacity', 'Current_load', 'Current_measured', 'Temperature_measured', 'Time', 'Voltage_load', 'Voltage_measured']\n",
    "        elif cycle_type == 'impedance':\n",
    "            columns = ['Battery_current', 'Battery_impedance', 'Current_ratio', 'Rct', 'Re', 'Rectified_Impedance', 'Sense_current']\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "        arr_lens = []\n",
    "        for col in columns:\n",
    "            arr = getattr(data, col, None)\n",
    "            if arr is not None and hasattr(arr, '__len__'):\n",
    "                arr_lens.append(len(arr))\n",
    "        if not arr_lens:\n",
    "            continue\n",
    "        min_len = min(arr_lens)\n",
    "\n",
    "        for idx in range(min_len):\n",
    "            row = {\n",
    "                'battery_name': battery_name,\n",
    "                'cycle': i,\n",
    "                'type': cycle_type,\n",
    "                'ambient_temperature': getattr(cycle, 'ambient_temperature', None),\n",
    "                'time': datetime.datetime(*[int(x) for x in getattr(cycle, 'time', [9999,12,31,0,0,0])])\n",
    "            }\n",
    "            \n",
    "            if cycle_type == 'impedance':\n",
    "                for col in columns:\n",
    "                    arr = getattr(data, col, None)\n",
    "                    value = arr[idx] if arr is not None and hasattr(arr, '__getitem__') else None\n",
    "                    row[col] = np.real(value) if value is not None else None\n",
    "            else:\n",
    "                for col in columns:\n",
    "                    arr = getattr(data, col, None)\n",
    "                    value = arr[idx] if arr is not None and hasattr(arr, '__getitem__') else None\n",
    "                    row[col] = value\n",
    "            rows.append(row)\n",
    "\n",
    "    df = pd.DataFrame(rows)\n",
    "    df.to_csv(export_filename, index=False)\n",
    "    print(f\"Exported flattened DataFrame to {export_filename}\")\n",
    "    return df\n",
    "\n",
    "# Example usage:\n",
    "# export_cycles_flattened(cycles, 'Flattened_b0006.csv', N=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84ff9bff",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = export_cycles_flattened('b0007', cycles, 'Flattened_b0007.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7f62846",
   "metadata": {},
   "outputs": [],
   "source": [
    "battery_objs = {\n",
    "    'b0005': b0005.cycle,\n",
    "    'b0006': b0006.cycle,\n",
    "    'b0007': b0007.cycle,\n",
    "    'b0018': b0018.cycle\n",
    "}\n",
    "\n",
    "for name, cycles in battery_objs.items():\n",
    "    csv_name = f'Flattened_{name}.csv'\n",
    "    df = export_cycles_flattened(name, cycles, csv_name)\n",
    "    print(f'Exported {csv_name}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db2e41a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a91ebb4b",
   "metadata": {},
   "source": [
    "### Amazon s3 Connections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6d5c0cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "This cell loads AWS credentials from a .env file using python-dotenv,\n",
    "creates a boto3 S3 client with those credentials,\n",
    "and lists all S3 buckets in the account.\n",
    "'''\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import boto3\n",
    "\n",
    "# Load environment variables from .env\n",
    "load_dotenv()\n",
    "\n",
    "# Create boto3 client using loaded environment variables\n",
    "s3 = boto3.client(\"s3\",\n",
    "    aws_access_key_id=os.getenv(\"AWS_ACCESS_KEY_ID\"),\n",
    "    aws_secret_access_key=os.getenv(\"AWS_SECRET_ACCESS_KEY\"),\n",
    "    region_name=os.getenv(\"AWS_DEFAULT_REGION\")\n",
    ")\n",
    "\n",
    "# Example: list buckets\n",
    "buckets = s3.list_buckets()\n",
    "print(\"Your S3 Buckets:\")\n",
    "for bucket in buckets['Buckets']:\n",
    "    print(f\" - {bucket['Name']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93d69be5",
   "metadata": {},
   "source": [
    "### Data Load from csv to Amazon s3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6ad5858",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "def load_files_from_directory(directory_path, file_types=('csv', 'xls', 'xlsx')):\n",
    "    \"\"\"\n",
    "    Load and append CSV and Excel files from a given directory into a single pandas DataFrame.\n",
    "\n",
    "    Args:\n",
    "        directory_path (str): Path to the directory containing the files.\n",
    "        file_types (tuple): File types to load. Default is ('csv', 'xls', 'xlsx').\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A single DataFrame containing data from all files.\n",
    "    \"\"\"\n",
    "    dataframes = []\n",
    "    for filename in os.listdir(directory_path):\n",
    "        filepath = os.path.join(directory_path, filename)\n",
    "        if os.path.isfile(filepath):\n",
    "            ext = filename.lower().split('.')[-1]\n",
    "            try:\n",
    "                if ext == 'csv' and 'csv' in file_types:\n",
    "                    df = pd.read_csv(filepath)\n",
    "                    dataframes.append(df)\n",
    "                elif ext in ['xls', 'xlsx'] and ext in file_types:\n",
    "                    df = pd.read_excel(filepath)\n",
    "                    dataframes.append(df)\n",
    "            except Exception as e:\n",
    "                print(f\"Failed to load {filename}: {e}\")\n",
    "    if dataframes:\n",
    "        return pd.concat(dataframes, ignore_index=True)\n",
    "    else:\n",
    "        return pd.DataFrame()  # Return an empty DataFrame if no files are loaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2d73eee",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = r\"G:\\DIYguru\\Data-Science-and-Engineering-Analytics\\Capstone_Project\\jupyter_notebooks\"\n",
    "DATA_DIR = load_files_from_directory(DATA_DIR)\n",
    "\n",
    "print(f\"Combined DataFrame shape: {DATA_DIR.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2faca3ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket_name = 's3aravindh973515031797'\n",
    "\n",
    "response = s3.list_objects_v2(Bucket=bucket_name)\n",
    "for item in response.get(\"Contents\", []):\n",
    "    print(item[\"Key\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c6b709c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def push_df_to_s3_parquet(df, object_key):\n",
    "    \"\"\"\n",
    "    Pushes a pandas DataFrame to a predefined S3 bucket as a Parquet file.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): DataFrame to upload.\n",
    "        object_key (str): S3 object key (path/filename.parquet).\n",
    "    \"\"\"\n",
    "    from io import BytesIO\n",
    "    parquet_buffer = BytesIO()\n",
    "    df.to_parquet(parquet_buffer, index=False)\n",
    "    parquet_buffer.seek(0)\n",
    "    s3.upload_fileobj(parquet_buffer, bucket_name, object_key)\n",
    "    print(f\"DataFrame uploaded to s3://{bucket_name}/{object_key}\")\n",
    "\n",
    "# Example usage:\n",
    "# push_df_to_s3_parquet(df_static, 'path/to/df_static.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5db02ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "push_df_to_s3_parquet(DATA_DIR, 'EV_Battery_Health_Source/EV_Battery_Health_Source.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d612d54e",
   "metadata": {},
   "source": [
    "### Data Load from Amazon s3 bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9ed4653",
   "metadata": {},
   "outputs": [],
   "source": [
    "from io import BytesIO\n",
    "\n",
    "def read_parquet_from_s3(bucket_name, object_key):\n",
    "        \"\"\"\n",
    "        Reads a Parquet file from an AWS S3 bucket using the global s3 client.\n",
    "\n",
    "        Args:\n",
    "            bucket_name: Name of the S3 bucket.\n",
    "            object_key: Key (path) to the Parquet file in the S3 bucket.\n",
    "\n",
    "        Returns:\n",
    "            DataFrame containing the Parquet data.\n",
    "        \"\"\"\n",
    "        response = s3.get_object(Bucket=bucket_name, Key=object_key)\n",
    "        file_content = response['Body'].read()\n",
    "        df = pd.read_parquet(BytesIO(file_content))\n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d05802ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "#del df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a9543ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket_name = 's3aravindh973515031797'\n",
    "df = 'EV_Battery_Health_Source/EV_Battery_Health_Source.parquet'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9de8a806",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = read_parquet_from_s3(bucket_name, df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f84b86a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e10c8d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9392f8b",
   "metadata": {},
   "source": [
    "### Exploratory Data Analysis (EDA)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edc9563d",
   "metadata": {},
   "source": [
    "### Battery Basics: Charge, Discharge & Impedance (Simple Definitions)\n",
    "\n",
    "| Term | Meaning | EV Car Analogy |\n",
    "|------|---------|----------------|\n",
    "| **Charge** | Process of **storing energy** in the battery by supplying electrical power. | Plugging your EV into a charging station — electricity flows in and refills the \"energy tank.\" |\n",
    "| **Discharge** | Process of **using the stored energy** to power a device or motor. | When you drive the EV, the stored battery energy powers the wheels and systems. |\n",
    "| **Impedance** | The **resistance to the flow of current** inside the battery, including both electrical and chemical reactions. Higher impedance reduces performance. | If your EV’s internal components become harder for electricity to flow through (like a clogged fuel line in petrol cars), acceleration and range drop. |\n",
    "\n",
    "**Low impedance** = better efficiency, more power delivery  \n",
    "**High impedance** = aging battery, heat generation, reduced range\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "296ace66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# unique battery name\n",
    "\n",
    "print(\"battery_name \\n\",df['battery_name'].unique())\n",
    "print(\"type \\n\",df['type'].unique())\n",
    "print(\"cycle \\n\",df['cycle'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bf3bddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['battery_name','type','cycle','Capacity','Voltage_measured','Current_measured','Temperature_measured']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9043ff1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['type'] == 'charge'][['battery_name','type','cycle','Capacity','Current_charge', 'Current_measured', 'Temperature_measured', 'Time', 'Voltage_charge', 'Voltage_measured']].head()\n",
    "\n",
    "#['Current_charge', 'Current_measured', 'Temperature_measured', 'Time', 'Voltage_charge', 'Voltage_measured']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1f26b55",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['type'] == 'discharge'][['battery_name','type','cycle','Capacity', 'Current_load', 'Current_measured', 'Temperature_measured', 'Time', 'Voltage_load', 'Voltage_measured']].head()\n",
    "\n",
    "#['Capacity', 'Current_load', 'Current_measured', 'Temperature_measured', 'Time', 'Voltage_load', 'Voltage_measured']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f64ea02",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['type'] == 'impedance'][['battery_name','type','cycle','Capacity','Battery_current', 'Battery_impedance', 'Current_ratio', 'Rct', 'Re', 'Rectified_Impedance', 'Sense_current']].head()\n",
    "\n",
    "#['Battery_current', 'Battery_impedance', 'Current_ratio', 'Rct', 'Re', 'Rectified_Impedance', 'Sense_current']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14bb7168",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "030f28f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#del df_cap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1f0a041",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cap = df[df['battery_name'] == 'b0005']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f9a1f8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cap.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca130372",
   "metadata": {},
   "source": [
    "### Capacity Degradation Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "711a6058",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# Convert the time column to datetime\n",
    "#df_cap['Time'] = pd.to_datetime(df_cap['Time'], errors='coerce')\n",
    "\n",
    "# Convert the float column to time\n",
    "df_cap['Time'] = pd.to_timedelta(df_cap['Time'],unit='s')\n",
    "\n",
    "# Sort to ensure correct sequence per cycle\n",
    "df_cap = df_cap.sort_values(['cycle', 'Time'])\n",
    "\n",
    "# Compute time difference (in seconds) per cycle\n",
    "df_cap['delta_t'] = df_cap.groupby('cycle')['Time'].diff().dt.total_seconds().fillna(0)\n",
    "\n",
    "# Compute instantaneous capacity in Ah = (|I| * Δt) / 3600\n",
    "df_cap['capacity_estimated'] = (df_cap['Current_measured'].abs() * df_cap['delta_t']) / 3600\n",
    "\n",
    "# Compute cumulative capacity per cycle\n",
    "df_cap['cumulative_capacity'] = df_cap.groupby('cycle')['capacity_estimated'].cumsum()\n",
    "\n",
    "# Take only discharge cycles (since they represent usable capacity)\n",
    "capacity_degradation = (\n",
    "    df_cap[df_cap['type'] == 'discharge']\n",
    "    .groupby('cycle')['cumulative_capacity']\n",
    "    .max()\n",
    "    .reset_index()\n",
    "    .rename(columns={'cumulative_capacity': 'capacity_Ah'})\n",
    ")\n",
    "\n",
    "# Plot Capacity Degradation Curve\n",
    "plt.figure(figsize=(8,5))\n",
    "plt.plot(capacity_degradation['cycle'], capacity_degradation['capacity_Ah'], marker='o')\n",
    "plt.title('Battery Capacity Degradation Curve (B0005)')\n",
    "plt.xlabel('Cycle Number')\n",
    "plt.ylabel('Capacity (Ah)')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Display summary\n",
    "print(capacity_degradation)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0ec1c74",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cap.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f26057e3",
   "metadata": {},
   "source": [
    "### Understanding df['delta_t'] = df.groupby(['battery_name', 'cycle'])['Time'].diff().dt.total_seconds().fillna(0)\n",
    "\n",
    "This line calculates the **time difference (Δt in seconds)** between consecutive measurements **within each battery and cycle**.\n",
    "\n",
    "**What it does:**\n",
    "- Groups data by `battery_name` and `cycle`\n",
    "- Computes time difference between the current and previous timestamp (`diff()`)\n",
    "- Converts the time difference to seconds (`dt.total_seconds()`)\n",
    "- Replaces the first value in each group with `0` (`fillna(0)`)\n",
    "\n",
    "**Example**\n",
    "\n",
    "| battery_name | cycle | Time                | delta_t |\n",
    "|--------------|--------|----------------------|---------|\n",
    "| B0005        | 1      | 2020-01-01 00:00:00  | 0       |\n",
    "| B0005        | 1      | 2020-01-01 00:00:10  | 10      |\n",
    "| B0005        | 1      | 2020-01-01 00:00:25  | 15      |\n",
    "| B0005        | 2      | 2020-01-01 00:00:00  | 0       |\n",
    "| B0005        | 2      | 2020-01-01 00:00:05  | 5       |\n",
    "| B0006        | 1      | 2020-01-01 00:00:00  | 0       |\n",
    "| B0006        | 1      | 2020-01-01 00:00:20  | 20      |\n",
    "\n",
    "**Meaning:**  \n",
    "Δt gives the **time step between rows** for each battery-cycle combination, useful for computing energy, capacity, and degradation metrics.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fa9dc5a",
   "metadata": {},
   "source": [
    "### Understanding df['capacity_estimated'] = (df['Current_measured'].abs() * df['delta_t']) / 3600\n",
    "\n",
    "This line computes the **incremental capacity** (in ampere-hours, Ah) contributed by each row.\n",
    "\n",
    "- Formula\n",
    "The capacity for a time step is:\n",
    "\n",
    "$$\n",
    "\\text{capacity\\_estimated (Ah)} \\;=\\; \\frac{|I|\\ (\\text{A}) \\times \\Delta t\\ (\\text{s})}{3600\\ (\\text{s/hour})}\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- \\(I\\) = `Current_measured` (A). We use \\(|I|\\) because capacity is the magnitude of charge transferred.\n",
    "- \\(\\Delta t\\) = `delta_t` (s), the time difference between consecutive measurements.\n",
    "- Dividing by 3600 converts ampere-seconds to ampere-hours.\n",
    "\n",
    "- Example\n",
    "- If `Current_measured = -2.0` A and `delta_t = 10` s:\n",
    "\n",
    "$$\n",
    "\\text{capacity} = \\frac{|{-2.0}|\\times 10}{3600} = \\frac{20}{3600} = 0.005555\\ldots\\ \\text{Ah} \\approx 0.00556\\ \\text{Ah}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17e609f1",
   "metadata": {},
   "source": [
    "### Understanding df['cumulative_capacity'] = df.groupby(['battery_name', 'cycle'])['capacity_estimated'].cumsum()\n",
    "\n",
    "This line computes the **running (cumulative) capacity** for each battery and cycle.\n",
    "\n",
    "- What it does:\n",
    "- Groups the data by `battery_name` and `cycle`\n",
    "- Takes the incremental capacity (`capacity_estimated`) for each row\n",
    "- Computes a **cumulative sum** (`cumsum()`) within each group\n",
    "\n",
    "Mathematically, for each row \\(k\\):\n",
    "\n",
    "$$\n",
    "\\text{cumulative\\_capacity}_k \\;=\\; \\sum_{i=1}^{k} \\text{capacity\\_estimated}_i\n",
    "$$\n",
    "\n",
    "- Why this is useful:\n",
    "- It shows how the total capacity accumulates over the duration of a cycle.\n",
    "- The final value of the cumulative sum for a cycle is the **total capacity of that cycle**.\n",
    "\n",
    "- Example\n",
    "\n",
    "| Row | capacity_estimated (Ah) | cumulative_capacity (Ah) |\n",
    "|-----|--------------------------|---------------------------|\n",
    "| 1   | 0.002                    | 0.002                     |\n",
    "| 2   | 0.003                    | 0.005                     |\n",
    "| 3   | 0.004                    | 0.009                     |\n",
    "\n",
    "- Usage:\n",
    "- Helpful for plotting **capacity vs time**, **capacity fade**, or **cycle degradation**.\n",
    "- The final cumulative value at the end of each cycle can be extracted as the cycle capacity.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ebbd45d",
   "metadata": {},
   "source": [
    "### Plot capacity vs. cycle number for all batteries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36bd2789",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Convert to datetime\n",
    "#df['Time'] = pd.to_datetime(df['Time'], errors='coerce')\n",
    "\n",
    "# Convert the float column to time\n",
    "df['Time'] = pd.to_timedelta(df['Time'],unit='s')\n",
    "\n",
    "# Sort properly\n",
    "df = df.sort_values(['battery_name', 'cycle', 'Time'])\n",
    "\n",
    "# Compute delta time per battery per cycle\n",
    "df['delta_t'] = df.groupby(['battery_name', 'cycle'])['Time'].diff().dt.total_seconds().fillna(0)\n",
    "\n",
    "# Compute instantaneous Ah\n",
    "df['capacity_estimated'] = (df['Current_measured'].abs() * df['delta_t']) / 3600\n",
    "\n",
    "# Compute cumulative capacity per cycle per battery\n",
    "df['cumulative_capacity'] = df.groupby(['battery_name', 'cycle'])['capacity_estimated'].cumsum()\n",
    "\n",
    "# Extract only DISCHARGE cycles — true usable capacity\n",
    "cap_deg = (\n",
    "    df[df['type'] == 'discharge']\n",
    "    .groupby(['battery_name', 'cycle'])['cumulative_capacity']\n",
    "    .max()\n",
    "    .reset_index()\n",
    "    .rename(columns={'cumulative_capacity': 'capacity_Ah'})\n",
    ")\n",
    "\n",
    "# Plot all batteries\n",
    "plt.figure(figsize=(10,6))\n",
    "\n",
    "for b in cap_deg['battery_name'].unique():\n",
    "    temp = cap_deg[cap_deg['battery_name'] == b]\n",
    "    plt.plot(temp['cycle'], temp['capacity_Ah'], marker='o', label=b.upper())\n",
    "\n",
    "plt.title(\"Battery Capacity Degradation Comparison\")\n",
    "plt.xlabel(\"Cycle Number\")\n",
    "plt.ylabel(\"Capacity (Ah)\")\n",
    "plt.grid(True)\n",
    "plt.legend(title=\"Battery\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d665fe6c",
   "metadata": {},
   "source": [
    "**=== Original DataFrame ===**\n",
    "\n",
    "| battery_name | cycle | Time                | Current_measured | type      |\n",
    "|--------------|-------|---------------------|------------------|-----------|\n",
    "| B0005        | 1     | 2020-01-01 00:00:00 | -1.5             | charge    |\n",
    "| B0005        | 1     | 2020-01-01 00:00:10 | -1.8             | charge    |\n",
    "| B0005        | 1     | 2020-01-01 00:00:25 | 0.5              | discharge |\n",
    "| B0005        | 2     | 2020-01-02 00:00:00 | -2.0             | charge    |\n",
    "| B0005        | 2     | 2020-01-02 00:00:20 | 2.0              | discharge |\n",
    "| B0006        | 1     | 2020-01-01 00:00:00 | 1.0              | discharge |\n",
    "| B0006        | 1     | 2020-01-01 00:00:20 | 1.2              | discharge |\n",
    "\n",
    "---\n",
    "\n",
    "**=== After Converting Time to datetime ===**\n",
    "\n",
    "| battery_name | cycle | Time                | Current_measured | type      |\n",
    "|--------------|-------|---------------------|------------------|-----------|\n",
    "| B0005        | 1     | 2020-01-01 00:00:00 | -1.5             | charge    |\n",
    "| B0005        | 1     | 2020-01-01 00:00:10 | -1.8             | charge    |\n",
    "| B0005        | 1     | 2020-01-01 00:00:25 | 0.5              | discharge |\n",
    "| B0005        | 2     | 2020-01-02 00:00:00 | -2.0             | charge    |\n",
    "| B0005        | 2     | 2020-01-02 00:00:20 | 2.0              | discharge |\n",
    "| B0006        | 1     | 2020-01-01 00:00:00 | 1.0              | discharge |\n",
    "| B0006        | 1     | 2020-01-01 00:00:20 | 1.2              | discharge |\n",
    "\n",
    "---\n",
    "\n",
    "**=== After Sorting ===**\n",
    "\n",
    "| battery_name | cycle | Time                | Current_measured | type      |\n",
    "|--------------|-------|---------------------|------------------|-----------|\n",
    "| B0005        | 1     | 2020-01-01 00:00:00 | -1.5             | charge    |\n",
    "| B0005        | 1     | 2020-01-01 00:00:10 | -1.8             | charge    |\n",
    "| B0005        | 1     | 2020-01-01 00:00:25 | 0.5              | discharge |\n",
    "| B0005        | 2     | 2020-01-02 00:00:00 | -2.0             | charge    |\n",
    "| B0005        | 2     | 2020-01-02 00:00:20 | 2.0              | discharge |\n",
    "| B0006        | 1     | 2020-01-01 00:00:00 | 1.0              | discharge |\n",
    "| B0006        | 1     | 2020-01-01 00:00:20 | 1.2              | discharge |\n",
    "\n",
    "---\n",
    "\n",
    "**=== After Computing `delta_t` ===**\n",
    "\n",
    "| battery_name | cycle | Time                | Current_measured | type      | delta_t |\n",
    "|--------------|-------|---------------------|------------------|-----------|---------|\n",
    "| B0005        | 1     | 2020-01-01 00:00:00 | -1.5             | charge    | 0.0     |\n",
    "| B0005        | 1     | 2020-01-01 00:00:10 | -1.8             | charge    | 10.0    |\n",
    "| B0005        | 1     | 2020-01-01 00:00:25 | 0.5              | discharge | 15.0    |\n",
    "| B0005        | 2     | 2020-01-02 00:00:00 | -2.0             | charge    | 0.0     |\n",
    "| B0005        | 2     | 2020-01-02 00:00:20 | 2.0              | discharge | 20.0    |\n",
    "| B0006        | 1     | 2020-01-01 00:00:00 | 1.0              | discharge | 0.0     |\n",
    "| B0006        | 1     | 2020-01-01 00:00:20 | 1.2              | discharge | 20.0    |\n",
    "\n",
    "---\n",
    "\n",
    "**=== After Computing `capacity_estimated` ===**\n",
    "\n",
    "| battery_name | cycle | Time                | Current_measured | type      | delta_t | capacity_estimated |\n",
    "|--------------|-------|---------------------|------------------|-----------|---------|---------------------|\n",
    "| B0005        | 1     | 2020-01-01 00:00:00 | -1.5             | charge    | 0.0     | 0.000000            |\n",
    "| B0005        | 1     | 2020-01-01 00:00:10 | -1.8             | charge    | 10.0    | 0.005000            |\n",
    "| B0005        | 1     | 2020-01-01 00:00:25 | 0.5              | discharge | 15.0    | 0.002083            |\n",
    "| B0005        | 2     | 2020-01-02 00:00:00 | -2.0             | charge    | 0.0     | 0.000000            |\n",
    "| B0005        | 2     | 2020-01-02 00:00:20 | 2.0              | discharge | 20.0    | 0.011111            |\n",
    "| B0006        | 1     | 2020-01-01 00:00:00 | 1.0              | discharge | 0.0     | 0.000000            |\n",
    "| B0006        | 1     | 2020-01-01 00:00:20 | 1.2              | discharge | 20.0    | 0.006667            |\n",
    "\n",
    "---\n",
    "\n",
    "**=== After Computing `cumulative_capacity` ===**\n",
    "\n",
    "| battery_name | cycle | Time                | Current_measured | type      | delta_t | capacity_estimated | cumulative_capacity |\n",
    "|--------------|-------|---------------------|------------------|-----------|---------|---------------------|----------------------|\n",
    "| B0005        | 1     | 2020-01-01 00:00:00 | -1.5             | charge    | 0.0     | 0.000000            | 0.000000             |\n",
    "| B0005        | 1     | 2020-01-01 00:00:10 | -1.8             | charge    | 10.0    | 0.005000            | 0.005000             |\n",
    "| B0005        | 1     | 2020-01-01 00:00:25 | 0.5              | discharge | 15.0    | 0.002083            | 0.007083             |\n",
    "| B0005        | 2     | 2020-01-02 00:00:00 | -2.0             | charge    | 0.0     | 0.000000            | 0.000000             |\n",
    "| B0005        | 2     | 2020-01-02 00:00:20 | 2.0              | discharge | 20.0    | 0.011111            | 0.011111             |\n",
    "| B0006        | 1     | 2020-01-01 00:00:00 | 1.0              | discharge | 0.0     | 0.000000            | 0.000000             |\n",
    "| B0006        | 1     | 2020-01-01 00:00:20 | 1.2              | discharge | 20.0    | 0.006667            | 0.006667             |\n",
    "\n",
    "---\n",
    "\n",
    "**=== Discharge Capacity Per Cycle (`cap_deg`) ===**\n",
    "\n",
    "| battery_name | cycle | capacity_Ah |\n",
    "|--------------|-------|-------------|\n",
    "| B0005        | 1     | 0.007083    |\n",
    "| B0005        | 2     | 0.011111    |\n",
    "| B0006        | 1     | 0.006667    |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3e21f9f",
   "metadata": {},
   "source": [
    "### Identify knee points where rapid degradation begins\n",
    "*A knee point is the cycle at which the battery transitions from slow, linear degradation to rapid exponential degradation.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bccbf43c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aaed94e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9cbe316",
   "metadata": {},
   "outputs": [],
   "source": [
    "from kneed import KneeLocator\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "battery_names = cap_deg['battery_name'].unique() # Use cap_deg for battery names\n",
    "\n",
    "knee_points = {}\n",
    "\n",
    "for battery in battery_names:\n",
    "    # Filter data for the current battery from the cap_deg DataFrame\n",
    "    battery_data = cap_deg[cap_deg['battery_name'] == battery].sort_values(by='cycle')\n",
    "    \n",
    "    # Get cycles and capacity (using the correct capacity_Ah)\n",
    "    cycles = battery_data['cycle'].values\n",
    "    capacity = battery_data['capacity_Ah'].values\n",
    "    \n",
    "    # Ensure there's enough data to find a knee\n",
    "    if len(cycles) < 5: # Kneed needs at least a few points\n",
    "        print(f\"Skipping {battery.upper()}: Not enough cycles ({len(cycles)}) for knee detection.\")\n",
    "        knee_points[battery] = None\n",
    "        continue\n",
    "\n",
    "    # Find the knee point\n",
    "    try:\n",
    "        knee = KneeLocator(\n",
    "            x=cycles,\n",
    "            y=capacity,\n",
    "            S=1.0, # Sensitivity parameter: Higher S means fewer knees.\n",
    "            curve='concave',\n",
    "            direction='decreasing',\n",
    "            online=True # Often works better for noisy data\n",
    "        )\n",
    "        \n",
    "        knee_points[battery] = knee.knee\n",
    "        \n",
    "        print(f\"Knee cycle for {battery.upper()}: {knee.knee}\")\n",
    "\n",
    "        # Plot for each battery\n",
    "        plt.figure(figsize=(8, 5))\n",
    "        plt.plot(cycles, capacity, marker='o', linestyle='-', label=f'{battery.upper()} Capacity')\n",
    "        if knee.knee is not None:\n",
    "            plt.axvline(x=knee.knee, color='r', linestyle='--', label=f'Knee Point at Cycle {int(knee.knee)}')\n",
    "            plt.plot(knee.knee, knee.knee_y, 'ro', markersize=8) # Mark the knee point on the curve\n",
    "        plt.xlabel(\"Cycle\")\n",
    "        plt.ylabel(\"Capacity (Ah)\")\n",
    "        plt.title(f'Knee Point Detection for Battery {battery.upper()}')\n",
    "        plt.grid(True)\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Could not find knee point for {battery.upper()}: {e}\")\n",
    "        knee_points[battery] = None\n",
    "\n",
    "print(\"\\nSummary of Knee Points:\")\n",
    "print(knee_points) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53072118",
   "metadata": {},
   "outputs": [],
   "source": [
    "from kneed import KneeLocator\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "battery_names = cap_deg['battery_name'].unique()\n",
    "knee_points = {}\n",
    "\n",
    "# Setup 2x2 subplots\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, battery in enumerate(battery_names[:4]):  # Limit to first 4 batteries\n",
    "    ax = axes[i]\n",
    "\n",
    "    battery_data = cap_deg[cap_deg['battery_name'] == battery].sort_values(by='cycle')\n",
    "    cycles = battery_data['cycle'].values\n",
    "    capacity = battery_data['capacity_Ah'].values\n",
    "\n",
    "    if len(cycles) < 5:\n",
    "        print(f\"Skipping {battery.upper()}: Not enough cycles ({len(cycles)}) for knee detection.\")\n",
    "        knee_points[battery] = None\n",
    "        continue\n",
    "\n",
    "    try:\n",
    "        knee = KneeLocator(\n",
    "            x=cycles,\n",
    "            y=capacity,\n",
    "            S=1.0,\n",
    "            curve='concave',\n",
    "            direction='decreasing',\n",
    "            online=True\n",
    "        )\n",
    "\n",
    "        knee_points[battery] = knee.knee\n",
    "        print(f\"Knee cycle for {battery.upper()}: {knee.knee}\")\n",
    "\n",
    "        # Plot in subplot\n",
    "        ax.plot(cycles, capacity, marker='o', linestyle='-', label=f'{battery.upper()}')\n",
    "        if knee.knee is not None:\n",
    "            ax.axvline(x=knee.knee, color='r', linestyle='--', label=f'Knee Point at Cycle {int(knee.knee)}')\n",
    "            ax.plot(knee.knee, knee.knee_y, 'o', markersize=8)\n",
    "\n",
    "        ax.set_title(f'{battery.upper()}')\n",
    "        ax.set_xlabel('Cycle')\n",
    "        ax.set_ylabel('Capacity (Ah)')\n",
    "        ax.grid(True)\n",
    "        ax.legend()\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Could not find knee for {battery.upper()}: {e}\")\n",
    "        knee_points[battery] = None\n",
    "\n",
    "# Hide unused subplots if <4 batteries\n",
    "for j in range(len(battery_names), 4):\n",
    "    fig.delaxes(axes[j])\n",
    "\n",
    "plt.suptitle('Knee Point Detection (2x2 Subplots)', fontsize=16)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nSummary of Knee Points:\")\n",
    "print(knee_points)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8a2d4c8",
   "metadata": {},
   "source": [
    "### Correlation Analysis - Heatmap of features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9111d6b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "discharge_df = df[df['type'] == 'discharge'].copy()\n",
    "\n",
    "features_df = discharge_df.groupby(['battery_name', 'cycle']).agg(\n",
    "    avg_voltage_measured=('Voltage_measured', 'mean'),\n",
    "    avg_current_measured=('Current_measured', 'mean'),\n",
    "    avg_temp_measured=('Temperature_measured', 'mean')\n",
    ").reset_index()\n",
    "\n",
    "# Merge the aggregated features with the capacity degradation data.\n",
    "# The 'cap_deg' DataFrame has the final capacity for each discharge cycle.\n",
    "final_df = pd.merge(\n",
    "    features_df,\n",
    "    cap_deg,\n",
    "    on=['battery_name', 'cycle']\n",
    ")\n",
    "\n",
    "correlation_cols = [\n",
    "    'capacity_Ah',\n",
    "    'avg_voltage_measured',\n",
    "    'avg_current_measured',\n",
    "    'avg_temp_measured',\n",
    "    'cycle' # Include cycle number to see its correlation with other features\n",
    "]\n",
    "corr_df = final_df[correlation_cols]\n",
    "\n",
    "correlation_matrix = corr_df.corr()\n",
    "\n",
    "plt.figure(figsize=(5, 3))\n",
    "sns.heatmap(\n",
    "    correlation_matrix,\n",
    "    #annot=True,      # Show the correlation values on the heatmap\n",
    "    cmap='coolwarm_r', # Use a color map that's good for correlations\n",
    "    fmt=\".2f\",       # Format the numbers to two decimal places\n",
    "    linewidths=.5\n",
    ")\n",
    "plt.title('Feature Correlation Heatmap')\n",
    "plt.show()\n",
    "\n",
    "# Display the correlation matrix as a table\n",
    "print(\"Correlation Matrix:\")\n",
    "print(correlation_matrix)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "197874cb",
   "metadata": {},
   "source": [
    "### Summary of Key Insights from Correlation Analysis\n",
    "\n",
    "The correlation heatmap provides a quantitative look at how different battery parameters relate to each other across all charging and discharging cycles. The most important relationships are those correlated with `capacity_Ah`, as this is our primary measure of battery health.\n",
    "\n",
    "*   **Capacity vs. Cycle Number (`-0.9` or lower)**:\n",
    "    *   **Observation**: There is a very strong *negative* correlation.\n",
    "    *   **Insight**: This confirms the fundamental principle of battery aging: as the number of cycles increases, the battery's capacity to hold a charge decreases. This is the primary degradation trend we want to model.\n",
    "\n",
    "*   **Capacity vs. Average Voltage (`+0.8` or higher)**:\n",
    "    *   **Observation**: There is a strong *positive* correlation.\n",
    "    *   **Insight**: Healthier batteries (with higher capacity) maintain a higher average voltage during their discharge cycle. This makes the average voltage a powerful and direct indicator of the battery's State of Health (SoH).\n",
    "\n",
    "*   **Capacity vs. Average Temperature (Moderate Negative Correlation)**:\n",
    "    *   **Observation**: There is a moderate *negative* correlation.\n",
    "    *   **Insight**: As batteries degrade, their internal resistance tends to increase, causing them to generate more heat. A higher average temperature during operation can therefore signal a decline in capacity and overall health.\n",
    "\n",
    "*   **Conclusion for Feature Selection**:\n",
    "    *   The analysis clearly identifies `cycle`, `avg_voltage_measured`, and `avg_temp_measured` as highly relevant features.\n",
    "    *   These features have a strong predictive relationship with `capacity_Ah` and should be used to build the SoH (State of Health) and RUL (Remaining Useful Life) models.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "509593dd",
   "metadata": {},
   "source": [
    "### Feature Redundancy (Dimensionality Reduction)\n",
    "\n",
    "*   **Objective**: To identify and potentially remove redundant predictive features to simplify the model and prevent multicollinearity.\n",
    "\n",
    "*   **Method**:\n",
    "    *   Examine the correlation heatmap for very high correlations (e.g., > 0.95 or < -0.95) between the predictor features themselves (i.e., excluding the target variable, `capacity_Ah`).\n",
    "    *   For instance, if `avg_voltage_measured` and `avg_current_measured` were correlated at `0.98`, they would be considered redundant as they provide similar information.\n",
    "\n",
    "*   **Action & Insight**:\n",
    "    *   If high redundancy is found, you can remove one of the highly correlated features to reduce the model's dimensionality.\n",
    "    *   **For this specific dataset**, the correlations between the sensor features (`avg_voltage_measured`, `avg_current_measured`, `avg_temp_measured`) are typically not high enough to be considered redundant. Therefore, all of them can be retained as valuable, independent predictors for the model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c16109e",
   "metadata": {},
   "source": [
    "### Outlier Detection Using the Z-score Method\n",
    "\n",
    "The Z-score tells us how many standard deviations away a data point is from the mean. A common threshold for identifying an outlier is a Z-score greater than 3 or less than -3. The formula is:\n",
    "\n",
    "Z = (x - μ) / σ\n",
    "\n",
    "- x: A single data point\n",
    "- μ: The mean of the data\n",
    "- σ: The standard deviation of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deb35a59",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Columns to check for outliers\n",
    "feature_cols_for_outliers = [\n",
    "    'capacity_Ah',\n",
    "    'avg_voltage_measured',\n",
    "    'avg_current_measured',\n",
    "    'avg_temp_measured'\n",
    "]\n",
    "\n",
    "# --- Z-Score ---\n",
    "# Select the subset of the DataFrame for outlier detection\n",
    "features_to_check = final_df[feature_cols_for_outliers]\n",
    "\n",
    "# Calculate Z-scores using vectorized operations (more efficient)\n",
    "z_scores = (features_to_check - features_to_check.mean()) / features_to_check.std()\n",
    "\n",
    "# Create a boolean mask to identify outliers where the absolute Z-score is > 3\n",
    "outlier_mask = np.abs(z_scores) > 3\n",
    "\n",
    "# Find rows that contain at least one outlier feature\n",
    "outlier_rows = final_df[outlier_mask.any(axis=1)]\n",
    "\n",
    "print(\"Outlier Detection using Z-score Method (|z| > 3)\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "if outlier_rows.empty:\n",
    "    print(\"No outliers found in the dataset.\")\n",
    "else:\n",
    "    print(f\"Found {len(outlier_rows)} rows with at least one outlier feature.\")\n",
    "    print(\"\\nRows identified as outliers:\")\n",
    "    print(outlier_rows)\n",
    "\n",
    "    # To see which specific features were outliers in these rows:\n",
    "    print(\"\\nDetails of the outlier Z-scores (>3):\")\n",
    "    print(z_scores[outlier_mask.any(axis=1)])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "555af935",
   "metadata": {},
   "source": [
    "- These outliers all belong to battery B0006 at the very end of its life (cycles 599, 603, and 607).\n",
    "- The specific feature that was an outlier was the avg_current_measured. Its Z-score was just slightly over 3.\n",
    "- 3 minor outliers in over 600 cycles tells you that the dataset is very clean and consistent\n",
    "- These outliers are not errors; they likely represent the battery's behavior becoming slightly erratic as it reached its absolute end-of-life."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ada56663",
   "metadata": {},
   "source": [
    "### Outlier management using IQR (If required)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca055393",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# --- Outlier Detection and Handling using IQR Method ---\n",
    "\n",
    "# 1. Define features to check (same as used in Z-score)\n",
    "feature_cols_for_outliers = [\n",
    "    'capacity_Ah',\n",
    "    'avg_voltage_measured',\n",
    "    'avg_current_measured',\n",
    "    'avg_temp_measured'\n",
    "]\n",
    "\n",
    "# 2. Calculate Q1 (25th percentile) and Q3 (75th percentile)\n",
    "Q1 = final_df[feature_cols_for_outliers].quantile(0.25)\n",
    "Q3 = final_df[feature_cols_for_outliers].quantile(0.75)\n",
    "\n",
    "# 3. Calculate IQR (Interquartile Range)\n",
    "IQR = Q3 - Q1\n",
    "\n",
    "# 4. Define Lower and Upper Bounds (1.5 * IQR rule)\n",
    "lower_bound = Q1 - 1.5 * IQR\n",
    "upper_bound = Q3 + 1.5 * IQR\n",
    "\n",
    "print(\"IQR Bounds for each feature:\")\n",
    "print(pd.DataFrame({'Lower Bound': lower_bound, 'Upper Bound': upper_bound}))\n",
    "\n",
    "# 5. Identify Outliers\n",
    "# A row is considered an outlier if ANY of its features fall outside the bounds\n",
    "outlier_mask_iqr = (\n",
    "    (final_df[feature_cols_for_outliers] < lower_bound) |\n",
    "    (final_df[feature_cols_for_outliers] > upper_bound)\n",
    ").any(axis=1)\n",
    "\n",
    "outliers_iqr = final_df[outlier_mask_iqr]\n",
    "\n",
    "print(f\"\\nNumber of outliers detected using IQR: {len(outliers_iqr)}\")\n",
    "print(f\"Percentage of dataset: {len(outliers_iqr) / len(final_df) * 100:.2f}%\")\n",
    "\n",
    "if not outliers_iqr.empty:\n",
    "    print(\"\\nSample of IQR Outliers:\")\n",
    "    print(outliers_iqr.head())\n",
    "\n",
    "# --- 6. Handling Strategy 1: Removal (Dropping Rows) ---\n",
    "\n",
    "#final_df_cleaned_iqr = final_df[~outlier_mask_iqr].copy()\n",
    "#print(f\"\\nShape after Removing Outliers: {final_df_cleaned_iqr.shape}\")\n",
    "\n",
    "# --- 7. Handling Strategy 2: Capping (Winsorization) ---\n",
    "\n",
    "final_df_capped = final_df.copy()\n",
    "for col in feature_cols_for_outliers:\n",
    "    final_df_capped[col] = final_df_capped[col].clip(lower=lower_bound[col], upper=upper_bound[col])\n",
    "\n",
    "print(\"Capping completed. Extreme values replaced with lower/upper bounds.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6481fd1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_to_check.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a31eee78",
   "metadata": {},
   "source": [
    "### Box Plots for Feature Distributions\n",
    "\n",
    "Box plots are useful for quickly identifying the median, quartiles, and outliers in your data.\n",
    "\n",
    "- **The Box**: Represents the interquartile range (IQR), which contains the middle 50% of the data (from the 25th to the 75th percentile).\n",
    "- **The Line Inside the Box**: Marks the median (50th percentile).\n",
    "- **The Whiskers**: Extend from the box to show the range of the data, typically up to 1.5 times the IQR.\n",
    "- **The Dots**: Individual points beyond the whiskers are potential outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88b8ef33",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "feature_cols_for_boxplot = [\n",
    "    'avg_voltage_measured',\n",
    "    'avg_current_measured',\n",
    "    'avg_temp_measured'\n",
    "]\n",
    "\n",
    "# Create a figure with subplots for each feature\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "\n",
    "# Plot for Average Voltage\n",
    "sns.boxplot(y=final_df['avg_voltage_measured'], ax=axes[0])\n",
    "axes[0].set_title('Voltage Distribution')\n",
    "axes[0].set_ylabel('Average Voltage (V)')\n",
    "\n",
    "# Plot for Average Current\n",
    "sns.boxplot(y=final_df['avg_current_measured'], ax=axes[1])\n",
    "axes[1].set_title('Current Distribution')\n",
    "axes[1].set_ylabel('Average Current (A)')\n",
    "\n",
    "# Plot for Average Temperature\n",
    "sns.boxplot(y=final_df['avg_temp_measured'], ax=axes[2])\n",
    "axes[2].set_title('Temperature Distribution')\n",
    "axes[2].set_ylabel('Average Temperature (°C)')\n",
    "\n",
    "# Adjust layout and display the plots\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bcbcaa3",
   "metadata": {},
   "source": [
    "- The plot for avg_current_measured will visually confirm what the Z-score test found: the data will be tightly packed in the box, but there are few dots representing the identified outlier points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59415d21",
   "metadata": {},
   "outputs": [],
   "source": [
    "discharge_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "830d009d",
   "metadata": {},
   "outputs": [],
   "source": [
    "discharge_df[['time','Time','delta_t']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d30a0fa2",
   "metadata": {},
   "source": [
    "### Feature Engineering: Electrochemical Features\n",
    "\n",
    "1. Voltage-Based Features:\n",
    "\n",
    "    -   Delta_V (Voltage Drop): The difference between the initial voltage at the start of the discharge and the final voltage at the end. A larger voltage drop can indicate higher internal resistance and degradation.\n",
    "\n",
    "2. Temperature-Based Features:\n",
    "\n",
    "    -   Delta_T (Temperature Increase): The difference between the maximum temperature and the initial temperature during discharge. As batteries age, their internal resistance increases, often leading to more heat generation.\n",
    "3. Time-Based Features:\n",
    "\n",
    "    -   Discharge_Time: The total duration of the discharge cycle in seconds. A shorter discharge time for the same load indicates a lower capacity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67a11edb",
   "metadata": {},
   "source": [
    "### Data Preprocessing: Feature Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "736525bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Compute SoH (Target Variable)\n",
    "\n",
    "initial_capacities = (\n",
    "    final_df.groupby('battery_name')['capacity_Ah']\n",
    "    .first()\n",
    "    .to_dict()\n",
    ")\n",
    "\n",
    "final_df['SoH'] = final_df['capacity_Ah'] / final_df['battery_name'].map(initial_capacities)\n",
    "\n",
    "# Define Features (X) and Target (y)\n",
    "\n",
    "features = [\n",
    "    'cycle',\n",
    "    'avg_voltage_measured',\n",
    "    'avg_current_measured',\n",
    "    'avg_temp_measured'\n",
    "]\n",
    "target = 'SoH'\n",
    "\n",
    "X = final_df[features]\n",
    "y = final_df[target]\n",
    "\n",
    "# Manual Split\n",
    "\n",
    "X_train = X[final_df['battery_name'] != 'b0018']\n",
    "y_train = y[final_df['battery_name'] != 'b0018']\n",
    "\n",
    "X_test  = X[final_df['battery_name'] == 'b0018']\n",
    "y_test  = y[final_df['battery_name'] == 'b0018']\n",
    "\n",
    "print(f\"Training Data Shape: {X_train.shape}\")\n",
    "print(f\"Testing Data Shape:  {X_test.shape}\")\n",
    "\n",
    "\n",
    "# MinMax Scaling (0–1 Normalization)\n",
    "\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "\n",
    "# Fit only on training data\n",
    "scaler.fit(X_train)\n",
    "\n",
    "# Transform both train and test\n",
    "X_train_scaled = scaler.transform(X_train)\n",
    "X_test_scaled  = scaler.transform(X_test)\n",
    "\n",
    "# Convert to DataFrames\n",
    "X_train_scaled_df = pd.DataFrame(X_train_scaled, columns=features, index=X_train.index)\n",
    "X_test_scaled_df  = pd.DataFrame(X_test_scaled,  columns=features, index=X_test.index)\n",
    "\n",
    "# OPTIONAL: Scale y (SoH) for neural networks\n",
    "\n",
    "# from sklearn.preprocessing import MinMaxScaler\n",
    "# y_scaler = MinMaxScaler()\n",
    "# y_train_scaled = y_scaler.fit_transform(y_train.values.reshape(-1,1))\n",
    "# y_test_scaled  = y_scaler.transform(y_test.values.reshape(-1,1))\n",
    "\n",
    "print(\"\\nOriginal Training Data (first 5 rows):\")\n",
    "print(X_train.head())\n",
    "\n",
    "print(\"\\nScaled Training Data (first 5 rows):\")\n",
    "print(X_train_scaled_df.head())\n",
    "\n",
    "print(\"\\nScaled Training Data Summary:\")\n",
    "print(X_train_scaled_df.describe())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2d8988f",
   "metadata": {},
   "source": [
    "- **Training:** B0005, B0006, B0007\n",
    "\n",
    "- **Testing:** B0018"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90a8ec45",
   "metadata": {},
   "source": [
    "### Supervised Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19a0ad35",
   "metadata": {},
   "source": [
    "Why Random Forest & XGBoost Are Preferred for Battery SoH Prediction\n",
    "\n",
    "1. **Non-Linear Relationship Handling**\n",
    "   - Battery degradation is non-linear.\n",
    "   - RF/XGBoost capture complex curves and interactions using tree ensembles.\n",
    "   - Linear Regression assumes straight-line relationships and performs poorly.\n",
    "   - Logistic Regression is not suitable because SoH is a continuous value.\n",
    "\n",
    "2. **High Predictive Performance & Robustness**\n",
    "   - Goal: Predict SoH within ±5% error.\n",
    "   - RF/XGBoost perform strongly on structured data and are less prone to overfitting.\n",
    "   - Robust to outliers; no heavy feature scaling required.\n",
    "   - KNN suffers when features use different scales; SVM requires heavy tuning.\n",
    "\n",
    "3. **Feature Importance & Explainability**\n",
    "   - Provide built-in feature importance to understand key drivers (e.g., cycle number, temperature).\n",
    "   - SVM and other models do not offer straightforward interpretability.\n",
    "\n",
    "Summary Table\n",
    "\n",
    "| Model | Strengths | Limitations |\n",
    "|-------|-----------|-------------|\n",
    "| **Random Forest** & **XGBoost** | Handles non-linear data well; strong accuracy; robust to noise; feature importance included | Requires more compute vs. linear models |\n",
    "| **Linear Regression** | Simple and fast | Fails on non-linear degradation trends |\n",
    "| **Logistic Regression** | Good for classification | Not suitable for continuous SoH prediction |\n",
    "| **KNN / SVM** | Can model non-linear patterns | Sensitive to scaling/tuning; less interpretable |\n",
    "\n",
    "\n",
    "**Conclusion:**  \n",
    "Random Forest and XGBoost deliver the best balance of accuracy, robustness, and interpretability for EV battery State-of-Health (SoH) prediction and are ideal baseline supervised learning models.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83497659",
   "metadata": {},
   "source": [
    "Summary of R², MAE, MSE, RMSE\n",
    "\n",
    "**1. One-liner Descriptions**\n",
    "- **R² (Coefficient of Determination):** Measures how well predictions match actual values.  \n",
    "  Formula: `R² = 1 − (Σ(y − ŷ)² / Σ(y − ȳ)²)`\n",
    "\n",
    "- **MAE (Mean Absolute Error):** Average of absolute prediction errors.  \n",
    "  Formula: `MAE = (1/n) Σ |y − ŷ|`\n",
    "\n",
    "- **MSE (Mean Squared Error):** Average of squared prediction errors.  \n",
    "  Formula: `MSE = (1/n) Σ(y − ŷ)²`\n",
    "\n",
    "- **RMSE (Root Mean Squared Error):** Square root of MSE; error in original units.  \n",
    "  Formula: `RMSE = √MSE`\n",
    "\n",
    "**2. Key Parameters**\n",
    "- `y` = actual value  \n",
    "- `ŷ` = predicted value  \n",
    "- `ȳ` = mean of actual values  \n",
    "- `n` = number of samples  \n",
    "- `e` = error (y − ŷ)\n",
    "\n",
    "**3. Simple Mathematical Meaning**\n",
    "- **MAE:** Average mistake size.  \n",
    "- **MSE:** Punishes big mistakes more.  \n",
    "- **RMSE:** MSE converted back to original scale.  \n",
    "- **R²:** Shows how much better the model is than predicting the average.\n",
    "\n",
    "**4. Summary Explanation**\n",
    "These metrics help evaluate model accuracy:  \n",
    "- MAE shows average error,  \n",
    "- MSE shows squared error,  \n",
    "- RMSE shows error on original scale,  \n",
    "- R² tells how well the model explains variance.\n",
    "\n",
    "**5. Cricket Analogy**\n",
    "Predicting a batsman’s runs:  \n",
    "- **MAE:** Average runs your guess was off.  \n",
    "- **MSE:** Big wrong guesses get punished more.  \n",
    "- **RMSE:** Error again shown in \"runs\".  \n",
    "- **R²:** How good your predicting method is vs always guessing average runs.\n",
    "\n",
    "**6. EV Analogy**\n",
    "Predicting EV driving range:  \n",
    "- **MAE:** Average km your prediction was off.  \n",
    "- **MSE:** Large range mistakes hurt more.  \n",
    "- **RMSE:** Error in km (easy to interpret).  \n",
    "- **R²:** How well prediction explains actual EV range behavior.\n",
    "\n",
    "**7. Real-World EV Example**\n",
    "Used in estimating **EV driving range** using factors like temperature, speed, battery age.  \n",
    "MAE/MSE/RMSE measure prediction error; R² shows overall model performance.\n",
    "\n",
    "**8. Example**\n",
    "\n",
    "Imagine we are predicting EV range. Actual Range: 100 km. Predicted: 90 km. Error: -10.MAE: \n",
    "- **MAE**: We take the absolute value (10). It tells us we are off by 10 km on average.\n",
    "- **MSE**: We square the error ($10^2 = 100$). This makes big errors look huge.\n",
    "- **RMSE**: We take the square root of 100 ($= 10$). It brings the number back to km so we can understand it.\n",
    "- **R²**: If the result is 0.90, it means our model explains 90% of the variations in range; it's like a grade out of 100%."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b74b90d",
   "metadata": {},
   "source": [
    "#### Random Forest Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3c8e38d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import joblib\n",
    "\n",
    "# --- 1. Initialize and Train the Random Forest Model ---\n",
    "\n",
    "rf_model = RandomForestRegressor(\n",
    "    n_estimators=100,  # The number of decision trees in the forest.\n",
    "    max_depth=10,      # The maximum depth of each tree.\n",
    "    random_state=42,   # Ensures reproducibility of the results.\n",
    "    n_jobs=-1          # Use all available CPU cores for faster training.\n",
    ")\n",
    "\n",
    "# Train the model on the scaled training data prepared in the previous step.\n",
    "# The model learns the relationship between the features and the State of Health (SoH).\n",
    "rf_model.fit(X_train_scaled_df, y_train)\n",
    "\n",
    "# --- 2. Make Predictions on the Test Set ---\n",
    "# Use the trained model to predict the SoH for the unseen test data (battery B0018).\n",
    "y_pred_rf = rf_model.predict(X_test_scaled_df)\n",
    "\n",
    "# --- 3. Evaluate Model Performance ---\n",
    "# Calculate key metrics to understand how well the model performed.\n",
    "rmse_rf = np.sqrt(mean_squared_error(y_test, y_pred_rf))\n",
    "mae_rf = mean_absolute_error(y_test, y_pred_rf)\n",
    "r2_rf = r2_score(y_test, y_pred_rf)\n",
    "\n",
    "print(\"--- Random Forest Regressor Performance ---\")\n",
    "print(f\"RMSE (Root Mean Squared Error): {rmse_rf:.4f}\")\n",
    "print(f\"MAE (Mean Absolute Error):      {mae_rf:.4f}\")\n",
    "print(f\"R² (R-squared):                 {r2_rf:.4f}\")\n",
    "\n",
    "# --- 4. Visualize Predictions vs. Actuals ---\n",
    "# Plotting the results helps to visually assess the model's accuracy.\n",
    "plt.figure(figsize=(12, 6))\n",
    "# We use the original X_test to get the cycle numbers for the x-axis.\n",
    "plt.plot(X_test['cycle'], y_test, label='Actual SoH', marker='o', linestyle='-')\n",
    "plt.plot(X_test['cycle'], y_pred_rf, label='Predicted SoH (Random Forest)', marker='x', linestyle='--')\n",
    "plt.title('Random Forest: Actual vs. Predicted SoH for Battery B0018')\n",
    "plt.xlabel('Cycle Number')\n",
    "plt.ylabel('State of Health (SoH)')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Save model\n",
    "joblib.dump(rf_model, \"rf_soh_model.joblib\")\n",
    "print(\"Saved RandomForest model as rf_soh_model.joblib\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e08afa1",
   "metadata": {},
   "source": [
    "-   **Mean Absolute Error (MAE)**: The MAE is 0.0170. This means that, on average, the model's SoH prediction is off by only 1.70%.\n",
    "-   **RMSE (Root Mean Squared Error)**: The RMSE is 0.0206, which is also very low, indicating that the prediction errors are consistently small.\n",
    "-   **R² Score:** The R² score will be about 0.9367, signifying that the model can explain over 94% of the variance in the battery's health, which is an excellent fit."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e483e73f",
   "metadata": {},
   "source": [
    "#### XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df3f3994",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "print(sys.executable)\n",
    "\n",
    "import xgboost\n",
    "print(xgboost.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e894d99",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# --- 1. Initialize and Train the XGBoost Model ---\n",
    "# Using parameters from the capstone guide for a baseline\n",
    "xgb_model = xgb.XGBRegressor(\n",
    "    n_estimators=100,\n",
    "    learning_rate=0.1,\n",
    "    max_depth=5,\n",
    "    random_state=42,\n",
    "    n_jobs=-1  # Use all available cores\n",
    ")\n",
    "\n",
    "# Train the model on the scaled training data\n",
    "xgb_model.fit(X_train_scaled_df, y_train)\n",
    "\n",
    "# --- 2. Make Predictions on the Test Set ---\n",
    "y_pred_xgb = xgb_model.predict(X_test_scaled_df)\n",
    "\n",
    "# --- 3. Evaluate Model Performance ---\n",
    "rmse_xgb = np.sqrt(mean_squared_error(y_test, y_pred_xgb))\n",
    "mae_xgb = mean_absolute_error(y_test, y_pred_xgb)\n",
    "r2_xgb = r2_score(y_test, y_pred_xgb)\n",
    "\n",
    "print(\"--- XGBoost Regressor Performance ---\")\n",
    "print(f\"RMSE: {rmse_xgb:.4f}\")\n",
    "print(f\"MAE:  {mae_xgb:.4f}\")\n",
    "print(f\"R²:   {r2_xgb:.4f}\")\n",
    "\n",
    "# --- 4. Compare with Random Forest ---\n",
    "# Create a DataFrame for a clear side-by-side comparison\n",
    "# Note: This assumes the variables rmse_rf, mae_rf, and r2_rf from the\n",
    "# previous cell are still in memory.\n",
    "performance_data = {\n",
    "    'Model': ['Random Forest', 'XGBoost'],\n",
    "    'RMSE': [rmse_rf, rmse_xgb],\n",
    "    'MAE': [mae_rf, mae_xgb],\n",
    "    'R²': [r2_rf, r2_xgb]\n",
    "}\n",
    "performance_df = pd.DataFrame(performance_data).set_index('Model')\n",
    "\n",
    "print(\"\\n--- Model Performance Comparison ---\")\n",
    "print(performance_df)\n",
    "\n",
    "# --- 5. Plotting the results helps to visually assess the model's accuracy.\n",
    "plt.figure(figsize=(12, 6))\n",
    "# We use the original X_test to get the cycle numbers for the x-axis.\n",
    "plt.plot(X_test['cycle'], y_test, label='Actual SoH', marker='o', linestyle='-')\n",
    "plt.plot(X_test['cycle'], y_pred_xgb, label='Predicted SoH (XGBoost)', marker='x', linestyle='--')\n",
    "plt.title('XGBoost: Actual vs. Predicted SoH for Battery B0018')\n",
    "plt.xlabel('Cycle Number')\n",
    "plt.ylabel('State of Health (SoH)')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# --- 6. Visualize the Comparison ---\n",
    "# A bar chart is great for comparing a single key metric like RMSE\n",
    "plt.figure(figsize=(8, 5))\n",
    "sns.barplot(x=performance_df.index, y=performance_df['RMSE'])\n",
    "plt.title('Model Comparison: Root Mean Squared Error (RMSE)')\n",
    "plt.ylabel('RMSE (lower is better)')\n",
    "\n",
    "# Add labels to the bars\n",
    "bars = plt.gca().patches\n",
    "for bar in bars:\n",
    "    yval = bar.get_height()\n",
    "    plt.text(bar.get_x() + bar.get_width()/2, yval, f'{yval:.4f}', \n",
    "             va='bottom', ha='center', fontsize=10, color='black')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Save model\n",
    "joblib.dump(xgb_model, \"xgb_model.joblib\")\n",
    "print(\"Saved XGBoost model as xgb_model.joblib\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48c81a96",
   "metadata": {},
   "source": [
    "**Summary of Model Performance**\n",
    "\n",
    "1. **Overall Accuracy is Very High**: Both models are highly accurate. The R² scores of 0.937 for Random Forest and 0.928 for XGBoost are outstanding. This means that your models can explain roughly 93-94% of the variance in the battery's State of Health (SoH) using the features you engineered. This confirms that your feature selection (cycle, avg_voltage_measured, etc.) was highly effective.\n",
    "\n",
    "2. **Random Forest is the Slight Winner**: In this head-to-head comparison, the Random Forest model performed slightly better across all metrics:\n",
    "\n",
    "    -   **Lower Error**: It has a lower RMSE (0.0206 vs. 0.0220) and a lower MAE (0.0170 vs. 0.0176), indicating its predictions were, on average, closer to the actual values.\n",
    "    -   **Better Fit**: Its R² score is slightly higher, meaning it captured the data's patterns a little more effectively than XGBoost.\n",
    "\n",
    "3. **Both Models Meet the Project Goal**: The primary objective is to predict SoH within a ±5% absolute error.\n",
    "\n",
    "    -   Random Forest model's Mean Absolute Error (MAE) is 1.70%.\n",
    "    -   Your XGBoost model's MAE is 1.76%.\n",
    "\n",
    "**Conclusion**\n",
    "\n",
    "While both models are excellent, the **Random Forest Regressor is the top-performing baseline model** for this specific task. It provides a slightly more accurate prediction of battery State of Health on the unseen test data (battery B0018)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e16af54d",
   "metadata": {},
   "source": [
    "### Deep Learning Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "687c2894",
   "metadata": {},
   "source": [
    "#### LSTM for Time-Series SoH Prediction\n",
    "\n",
    "**They Understand Sequences:** Unlike Random Forest or XGBoost which treat each cycle's data as an independent point, LSTMs are a type of Recurrent Neural Network (RNN) designed to recognize patterns in sequences. They can look at the data from cycle 1, then cycle 2, then cycle 3, and understand the trend of degradation over time\n",
    "\n",
    "**They Have \"Memory\":** The \"Long Short-Term Memory\" name is key. LSTMs have internal mechanisms called \"gates\" that allow them to remember important information from earlier in the sequence (long-term memory) and use it to make predictions for later time steps. This is perfect for battery health, where the SoH at cycle 100 is highly dependent on its history from cycles 1 through 99.\n",
    "\n",
    "**Excellent for Forecasting (RUL):** Because LSTMs learn the underlying degradation curve, they are not just good for predicting the SoH at the next time step; they are also powerful for forecasting the curve into the future to predict the Remaining Useful Life (RUL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb84f6f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "\n",
    "# --- 1. Prepare Data for LSTM ---\n",
    "# LSTMs expect data in the shape: [samples, timesteps, features]\n",
    "\n",
    "def create_sequences(X, y, time_steps=10):\n",
    "    \"\"\"\n",
    "    Creates sequences of data for time-series forecasting.\n",
    "    \"\"\"\n",
    "    Xs, ys = [], []\n",
    "    for i in range(len(X) - time_steps):\n",
    "        Xs.append(X.iloc[i:(i + time_steps)].values)\n",
    "        ys.append(y.iloc[i + time_steps])\n",
    "    return np.array(Xs), np.array(ys)\n",
    "\n",
    "# Define the number of past time steps to use for prediction\n",
    "TIME_STEPS = 10\n",
    "\n",
    "# Create sequences from the scaled training and testing data\n",
    "X_train_seq, y_train_seq = create_sequences(X_train_scaled_df, y_train, TIME_STEPS)\n",
    "X_test_seq, y_test_seq = create_sequences(X_test_scaled_df, y_test, TIME_STEPS)\n",
    "\n",
    "print(f\"Training sequence shape: {X_train_seq.shape}\")\n",
    "print(f\"Test sequence shape:     {X_test_seq.shape}\")\n",
    "\n",
    "\n",
    "# --- 2. Build the LSTM Model ---\n",
    "lstm_model = Sequential([\n",
    "    LSTM(64, activation='relu', return_sequences=True,\n",
    "         input_shape=(X_train_seq.shape[1], X_train_seq.shape[2])),\n",
    "    Dropout(0.2),\n",
    "    LSTM(32, activation='relu'),\n",
    "    Dropout(0.2),\n",
    "    Dense(16, activation='relu'),\n",
    "    Dense(1) # Output layer: predicts a single SoH value\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "lstm_model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
    "lstm_model.summary()\n",
    "\n",
    "# --- 3. Train the LSTM Model ---\n",
    "# We'll train for 50 epochs, which is a reasonable number for this task.\n",
    "history = lstm_model.fit(\n",
    "    X_train_seq, y_train_seq,\n",
    "    epochs=50,\n",
    "    batch_size=32,\n",
    "    validation_split=0.2, # Use part of the training data for validation\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# --- 4. Evaluate on Test Data and Predict RUL ---\n",
    "y_pred_lstm = lstm_model.predict(X_test_seq)\n",
    "\n",
    "# Calculate SoH prediction metrics\n",
    "rmse_lstm = np.sqrt(mean_squared_error(y_test_seq, y_pred_lstm))\n",
    "mae_lstm = mean_absolute_error(y_test_seq, y_pred_lstm)\n",
    "r2_lstm = r2_score(y_test_seq, y_pred_lstm)\n",
    "\n",
    "print(\"\\n--- LSTM SoH Prediction Performance ---\")\n",
    "print(f\"RMSE: {rmse_lstm:.4f}\")\n",
    "print(f\"MAE:  {mae_lstm:.4f}\")\n",
    "print(f\"R²:   {r2_lstm:.4f}\")\n",
    "\n",
    "\n",
    "# --- 5. Estimate RUL (Remaining Useful Life) ---\n",
    "# Industry failure threshold (e.g., 70% SoH)\n",
    "FAILURE_THRESHOLD = 0.70\n",
    "#FAILURE_THRESHOLD = 0.90\n",
    "\n",
    "# Find the cycle number where the actual SoH crosses the threshold\n",
    "actual_failure_cycle = np.where(y_test_seq < FAILURE_THRESHOLD)[0]\n",
    "if len(actual_failure_cycle) > 0:\n",
    "    actual_failure_cycle = y_test.index[actual_failure_cycle[0] + TIME_STEPS]\n",
    "else:\n",
    "    actual_failure_cycle = -1 # Did not fail in the test set\n",
    "\n",
    "# Find the cycle number where the predicted SoH crosses the threshold\n",
    "predicted_failure_cycle = np.where(y_pred_lstm < FAILURE_THRESHOLD)[0]\n",
    "if len(predicted_failure_cycle) > 0:\n",
    "    predicted_failure_cycle = y_test.index[predicted_failure_cycle[0] + TIME_STEPS]\n",
    "else:\n",
    "    predicted_failure_cycle = -1 # Predicted not to fail\n",
    "\n",
    "print(\"\\n--- RUL Estimation ---\")\n",
    "print(f\"Failure Threshold: {FAILURE_THRESHOLD*100:.0f}% SoH\")\n",
    "if actual_failure_cycle != -1:\n",
    "    print(f\"Actual Failure Cycle: {actual_failure_cycle}\")\n",
    "else:\n",
    "    print(\"Actual Failure: Battery did not reach threshold in test data.\")\n",
    "\n",
    "if predicted_failure_cycle != -1:\n",
    "    print(f\"Predicted Failure Cycle: {predicted_failure_cycle}\")\n",
    "else:\n",
    "    print(\"Predicted Failure: Model predicts battery will not fail in this dataset.\")\n",
    "\n",
    "# Save model\n",
    "joblib.dump(lstm_model, \"lstm_soh_model.joblib\")\n",
    "print(\"Saved RandomForest model as lstm_soh_model.joblib\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98f5897e",
   "metadata": {},
   "source": [
    "Layer-by-Layer Description\n",
    "\n",
    "**1. Input Sequence**\n",
    "\n",
    "Shape: (batch_size, 10, F)\n",
    "\n",
    "Meaning: 10 past cycles, each with F features (e.g., voltage, current, temp).\n",
    "\n",
    "**2. LSTM (64 units)**\n",
    "\n",
    "Produces 64 learned features per timestep.\n",
    "\n",
    "Captures short-term dependencies within the 10-cycle window.\n",
    "\n",
    "**3. Dropout**\n",
    "\n",
    "Randomly zeroes 20% of outputs.\n",
    "\n",
    "Avoids overfitting by forcing generalization.\n",
    "\n",
    "**4. LSTM (32 units, return_sequences=False)**\n",
    "\n",
    "Reads the full 10-timestep sequence.\n",
    "\n",
    "Outputs one final vector of length 32 summarizing the entire sequence.\n",
    "\n",
    "**5. Dropout**\n",
    "\n",
    "Again drops 20% of neurons.\n",
    "\n",
    "Protects against memorizing training data.\n",
    "\n",
    "**6. Dense(16)**\n",
    "\n",
    "Learns higher-level interactions among the 32 LSTM features.\n",
    "\n",
    "Prepares for final prediction.\n",
    "\n",
    "**7. Dense(1)**\n",
    "\n",
    "Outputs a single SoH prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b9042cb",
   "metadata": {},
   "source": [
    "**ReLU (Rectified Linear Unit):** ReLU is an activation function applied in Dense and sometimes LSTM layers. ReLU keeps positive values and converts negative values to zero.\n",
    "\n",
    "**Adam (Adaptive Moment Estimation):** Adam is an optimizer — it updates the model’s weights during training. It looks at the error at each step. It adjusts weights in the direction that reduces error.\n",
    "\n",
    "It uses:\n",
    "Momentum → avoids zig-zag\n",
    "Adaptive scaling → avoids big jumps\n",
    "\n",
    "***SGD (simple gradient descent) jumps around, Adam converges with stability.***\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "057bf7e8",
   "metadata": {},
   "source": [
    "#### RUL Classification Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "461f5a96",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, recall_score, precision_score, f1_score, classification_report\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# --- 1. Define RUL Classification Parameters ---\n",
    "RUL_WINDOW = 20  # Warning window: predict failure if RUL is <= 20 cycles\n",
    "FAILURE_THRESHOLD = 0.70 # SoH level defining failure\n",
    "#FAILURE_THRESHOLD = 0.90\n",
    "\n",
    "# --- 2. Generate Actual RUL Labels for the Test Set ---\n",
    "# Find the actual failure cycle in the test set\n",
    "actual_failure_cycle_idx = np.where(y_test_seq < FAILURE_THRESHOLD)[0]\n",
    "\n",
    "if len(actual_failure_cycle_idx) > 0:\n",
    "    actual_failure_cycle = actual_failure_cycle_idx[0]\n",
    "    \n",
    "    # Create the binary labels: 1 if within the window, 0 otherwise\n",
    "    y_test_rul_actual = np.zeros_like(y_test_seq)\n",
    "    # The start of the \"about to fail\" window\n",
    "    warning_start_index = max(0, actual_failure_cycle - RUL_WINDOW)\n",
    "    y_test_rul_actual[warning_start_index:actual_failure_cycle] = 1\n",
    "else:\n",
    "    # If the battery never fails in the test set, all labels are 0\n",
    "    y_test_rul_actual = np.zeros_like(y_test_seq)\n",
    "    actual_failure_cycle = len(y_test_seq) # Set to end for calculation\n",
    "    print(\"Battery did not fail in the test set. Recall cannot be fully evaluated.\")\n",
    "\n",
    "# --- 3. Generate Predicted RUL Labels ---\n",
    "# Find the predicted failure cycle\n",
    "predicted_failure_cycle_idx = np.where(y_pred_lstm.flatten() < FAILURE_THRESHOLD)[0]\n",
    "\n",
    "if len(predicted_failure_cycle_idx) > 0:\n",
    "    predicted_failure_cycle = predicted_failure_cycle_idx[0]\n",
    "    \n",
    "    # Create the binary labels for predictions\n",
    "    y_test_rul_pred = np.zeros_like(y_test_seq)\n",
    "    pred_warning_start_index = max(0, predicted_failure_cycle - RUL_WINDOW)\n",
    "    y_test_rul_pred[pred_warning_start_index:predicted_failure_cycle] = 1\n",
    "else:\n",
    "    # If the model predicts no failure, all labels are 0\n",
    "    y_test_rul_pred = np.zeros_like(y_test_seq)\n",
    "    predicted_failure_cycle = len(y_test_seq) # Set to end\n",
    "    print(\"Model did not predict a failure in the test set.\")\n",
    "\n",
    "# --- 4. Calculate and Display Classification Metrics ---\n",
    "# We can only calculate recall if there were actual positives to find\n",
    "if np.sum(y_test_rul_actual) > 0:\n",
    "    recall = recall_score(y_test_rul_actual, y_test_rul_pred)\n",
    "    precision = precision_score(y_test_rul_actual, y_test_rul_pred)\n",
    "    f1 = f1_score(y_test_rul_actual, y_test_rul_pred)\n",
    "    \n",
    "    print(f\"--- RUL Failure Detection Performance (W={RUL_WINDOW} cycles) ---\")\n",
    "    print(f\"Recall:    {recall:.2%}\")\n",
    "    print(f\"Precision: {precision:.2%}\")\n",
    "    print(f\"F1-Score:  {f1:.2f}\")\n",
    "    \n",
    "    # --- 5. Visualize the Confusion Matrix ---\n",
    "    cm = confusion_matrix(y_test_rul_actual, y_test_rul_pred)\n",
    "    plt.figure(figsize=(6, 5))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "                xticklabels=['Predicted Healthy', 'Predicted Failure'],\n",
    "                yticklabels=['Actual Healthy', 'Actual Failure'])\n",
    "    plt.title('RUL Failure Detection Confusion Matrix')\n",
    "    plt.ylabel('Actual Label')\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\n Classification Report:\")\n",
    "    print(classification_report(y_test_rul_actual, y_test_rul_pred, target_names=['Healthy', 'About to Fail']))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4c5d6e7",
   "metadata": {},
   "source": [
    "#### Adjusting Train/Test Split for RUL Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8g9h0i1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "from sklearn.metrics import recall_score, confusion_matrix, classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# --- 1. Create New Train/Test Split ---\n",
    "# We'll use B0007 for testing since it fails, and the rest for training.\n",
    "print('--- Creating New Train/Test Split for RUL Evaluation ---')\n",
    "train_batteries = ['b0005', 'b0006', 'b0018']\n",
    "test_battery = 'b0007'\n",
    "\n",
    "X_train_rul = final_df[final_df['battery_name'].isin(train_batteries)][features]\n",
    "y_train_rul = final_df[final_df['battery_name'].isin(train_batteries)][target]\n",
    "\n",
    "X_test_rul = final_df[final_df['battery_name'] == test_battery][features]\n",
    "y_test_rul = final_df[final_df['battery_name'] == test_battery][target]\n",
    "\n",
    "print(f\"Training on: {train_batteries}, Testing on: {test_battery}\")\n",
    "print(f\"New Training Data Shape: {X_train_rul.shape}\")\n",
    "print(f\"New Testing Data Shape:  {X_test_rul.shape}\")\n",
    "\n",
    "# --- 2. Rescale the Data ---\n",
    "scaler_rul = MinMaxScaler(feature_range=(0, 1))\n",
    "X_train_rul_scaled = scaler_rul.fit_transform(X_train_rul)\n",
    "X_test_rul_scaled = scaler_rul.transform(X_test_rul)\n",
    "\n",
    "# Convert back to DataFrame to keep column names and indices\n",
    "X_train_rul_scaled_df = pd.DataFrame(X_train_rul_scaled, columns=features, index=X_train_rul.index)\n",
    "X_test_rul_scaled_df = pd.DataFrame(X_test_rul_scaled, columns=features, index=X_test_rul.index)\n",
    "\n",
    "# --- 3. Create Sequences and Retrain LSTM ---\n",
    "TIME_STEPS = 10\n",
    "X_train_seq_rul, y_train_seq_rul = create_sequences(X_train_rul_scaled_df, y_train_rul, TIME_STEPS)\n",
    "X_test_seq_rul, y_test_seq_rul = create_sequences(X_test_rul_scaled_df, y_test_rul, TIME_STEPS)\n",
    "\n",
    "# Build and compile a new LSTM model for this specific evaluation\n",
    "lstm_model_rul = Sequential([\n",
    "    LSTM(64, activation='relu', return_sequences=True, input_shape=(X_train_seq_rul.shape[1], X_train_seq_rul.shape[2])),\n",
    "    Dropout(0.2),\n",
    "    LSTM(32, activation='relu'),\n",
    "    Dropout(0.2),\n",
    "    Dense(16, activation='relu'),\n",
    "    Dense(1)\n",
    "])\n",
    "lstm_model_rul.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
    "\n",
    "print('\\n--- Retraining LSTM Model ---')\n",
    "history_rul = lstm_model_rul.fit(\n",
    "    X_train_seq_rul, y_train_seq_rul,\n",
    "    epochs=50,\n",
    "    batch_size=32,\n",
    "    validation_split=0.2,\n",
    "    verbose=1 # Set to 1 if you want to see epoch-by-epoch training progress\n",
    ")\n",
    "print('Training complete.')\n",
    "\n",
    "# --- 4. Predict and Evaluate RUL Recall ---\n",
    "y_pred_lstm_rul = lstm_model_rul.predict(X_test_seq_rul).flatten()\n",
    "\n",
    "RUL_WINDOW = 20\n",
    "FAILURE_THRESHOLD = 0.70 # Using 70% as it's a common, earlier threshold\n",
    "#FAILURE_THRESHOLD = 0.90\n",
    "\n",
    "# Generate actual labels\n",
    "actual_failure_cycle_idx = np.where(y_test_seq_rul < FAILURE_THRESHOLD)[0]\n",
    "if len(actual_failure_cycle_idx) > 0:\n",
    "    actual_failure_cycle = actual_failure_cycle_idx[0]\n",
    "    y_test_rul_actual = np.zeros_like(y_test_seq_rul)\n",
    "    warning_start_index = max(0, actual_failure_cycle - RUL_WINDOW)\n",
    "    y_test_rul_actual[warning_start_index:actual_failure_cycle] = 1\n",
    "else:\n",
    "    y_test_rul_actual = np.zeros_like(y_test_seq_rul)\n",
    "    print(\"Warning: Battery in new test set did not fail below threshold.\")\n",
    "\n",
    "# Generate predicted labels\n",
    "predicted_failure_cycle_idx = np.where(y_pred_lstm_rul < FAILURE_THRESHOLD)[0]\n",
    "if len(predicted_failure_cycle_idx) > 0:\n",
    "    predicted_failure_cycle = predicted_failure_cycle_idx[0]\n",
    "    y_test_rul_pred = np.zeros_like(y_test_seq_rul)\n",
    "    pred_warning_start_index = max(0, predicted_failure_cycle - RUL_WINDOW)\n",
    "    y_test_rul_pred[pred_warning_start_index:predicted_failure_cycle] = 1\n",
    "else:\n",
    "    y_test_rul_pred = np.zeros_like(y_test_seq_rul)\n",
    "    print(\"Model did not predict a failure.\")\n",
    "\n",
    "# --- 5. Display RUL Classification Metrics ---\n",
    "if np.sum(y_test_rul_actual) > 0:\n",
    "    recall = recall_score(y_test_rul_actual, y_test_rul_pred)\n",
    "    print(f\"\\n--- RUL Failure Detection Performance (Threshold={FAILURE_THRESHOLD*100}%, Window={RUL_WINDOW} cycles) ---\")\n",
    "    print(f\"Recall:    {recall:.2%}\")\n",
    "\n",
    "    cm = confusion_matrix(y_test_rul_actual, y_test_rul_pred)\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "                xticklabels=['Predicted Healthy', 'Predicted Failure'],\n",
    "                yticklabels=['Actual Healthy', 'Actual Failure'])\n",
    "    plt.title('RUL Failure Detection Confusion Matrix (B0007)')\n",
    "    plt.ylabel('Actual Label')\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.show()\n",
    "\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(y_test_rul_actual, y_test_rul_pred, target_names=['Healthy', 'About to Fail'], zero_division=0))\n",
    "else:\n",
    "    print(\"\\nCould not evaluate recall as no actual failure events were found in the window.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "594ea0cb",
   "metadata": {},
   "source": [
    "#### INCREASED TIME_STEPS: Giving the model more history (30 cycles) to see the trend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93fd3d06",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "from sklearn.metrics import recall_score, confusion_matrix, classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# --- 1. Create New Train/Test Split ---\n",
    "# We'll use B0007 for testing since it fails, and the rest for training.\n",
    "print('--- Creating New Train/Test Split for RUL Evaluation ---')\n",
    "train_batteries = ['b0005', 'b0006', 'b0018']\n",
    "test_battery = 'b0007'\n",
    "\n",
    "# Assuming 'final_df' and 'features'/'target' variables are available from previous cells\n",
    "X_train_rul = final_df[final_df['battery_name'].isin(train_batteries)][features]\n",
    "y_train_rul = final_df[final_df['battery_name'].isin(train_batteries)][target]\n",
    "\n",
    "X_test_rul = final_df[final_df['battery_name'] == test_battery][features]\n",
    "y_test_rul = final_df[final_df['battery_name'] == test_battery][target]\n",
    "\n",
    "print(f\"Training on: {train_batteries}, Testing on: {test_battery}\")\n",
    "print(f\"New Training Data Shape: {X_train_rul.shape}\")\n",
    "print(f\"New Testing Data Shape:  {X_test_rul.shape}\")\n",
    "\n",
    "# --- 2. Rescale the Data ---\n",
    "scaler_rul = MinMaxScaler(feature_range=(0, 1))\n",
    "X_train_rul_scaled = scaler_rul.fit_transform(X_train_rul)\n",
    "X_test_rul_scaled = scaler_rul.transform(X_test_rul)\n",
    "\n",
    "# Convert back to DataFrame to keep column names and indices\n",
    "X_train_rul_scaled_df = pd.DataFrame(X_train_rul_scaled, columns=features, index=X_train_rul.index)\n",
    "X_test_rul_scaled_df = pd.DataFrame(X_test_rul_scaled, columns=features, index=X_test_rul.index)\n",
    "\n",
    "# --- 3. Create Sequences and Retrain LSTM ---\n",
    "# INCREASED TIME_STEPS: Giving the model more history (30 cycles) to see the trend\n",
    "TIME_STEPS = 30\n",
    "X_train_seq_rul, y_train_seq_rul = create_sequences(X_train_rul_scaled_df, y_train_rul, TIME_STEPS)\n",
    "X_test_seq_rul, y_test_seq_rul = create_sequences(X_test_rul_scaled_df, y_test_rul, TIME_STEPS)\n",
    "\n",
    "# Build and compile a new, deeper LSTM model\n",
    "lstm_model_rul = Sequential([\n",
    "    LSTM(128, activation='tanh', return_sequences=True, input_shape=(X_train_seq_rul.shape[1], X_train_seq_rul.shape[2])),\n",
    "    Dropout(0.2),\n",
    "    LSTM(64, activation='tanh'),\n",
    "    Dropout(0.2),\n",
    "    Dense(32, activation='relu'),\n",
    "    Dense(1)\n",
    "])\n",
    "lstm_model_rul.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
    "\n",
    "print('\\n--- Retraining LSTM Model (Improved Architecture) ---')\n",
    "history_rul = lstm_model_rul.fit(\n",
    "    X_train_seq_rul, y_train_seq_rul,\n",
    "    epochs=100,      # Increased epochs for better convergence\n",
    "    batch_size=16,   # Smaller batch size for more frequent updates\n",
    "    validation_split=0.2,\n",
    "    verbose=1\n",
    ")\n",
    "print('Training complete.')\n",
    "\n",
    "# --- 4. Predict and Evaluate RUL Recall ---\n",
    "y_pred_lstm_rul = lstm_model_rul.predict(X_test_seq_rul).flatten()\n",
    "\n",
    "RUL_WINDOW = 20\n",
    "FAILURE_THRESHOLD = 0.70 \n",
    "\n",
    "# Generate actual labels\n",
    "actual_failure_cycle_idx = np.where(y_test_seq_rul < FAILURE_THRESHOLD)[0]\n",
    "if len(actual_failure_cycle_idx) > 0:\n",
    "    actual_failure_cycle = actual_failure_cycle_idx[0]\n",
    "    y_test_rul_actual = np.zeros_like(y_test_seq_rul)\n",
    "    warning_start_index = max(0, actual_failure_cycle - RUL_WINDOW)\n",
    "    y_test_rul_actual[warning_start_index:actual_failure_cycle] = 1\n",
    "else:\n",
    "    y_test_rul_actual = np.zeros_like(y_test_seq_rul)\n",
    "    print(\"Warning: Battery in new test set did not fail below threshold.\")\n",
    "\n",
    "# Generate predicted labels\n",
    "predicted_failure_cycle_idx = np.where(y_pred_lstm_rul < FAILURE_THRESHOLD)[0]\n",
    "if len(predicted_failure_cycle_idx) > 0:\n",
    "    predicted_failure_cycle = predicted_failure_cycle_idx[0]\n",
    "    y_test_rul_pred = np.zeros_like(y_test_seq_rul)\n",
    "    pred_warning_start_index = max(0, predicted_failure_cycle - RUL_WINDOW)\n",
    "    y_test_rul_pred[pred_warning_start_index:predicted_failure_cycle] = 1\n",
    "else:\n",
    "    y_test_rul_pred = np.zeros_like(y_test_seq_rul)\n",
    "    print(\"Model did not predict a failure.\")\n",
    "\n",
    "# --- 5. Display RUL Classification Metrics ---\n",
    "if np.sum(y_test_rul_actual) > 0:\n",
    "    recall = recall_score(y_test_rul_actual, y_test_rul_pred)\n",
    "    print(f\"\\n--- RUL Failure Detection Performance (Threshold={FAILURE_THRESHOLD*100}%, Window={RUL_WINDOW} cycles) ---\")\n",
    "    print(f\"Recall:    {recall:.2%}\")\n",
    "\n",
    "    cm = confusion_matrix(y_test_rul_actual, y_test_rul_pred)\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "                xticklabels=['Predicted Healthy', 'Predicted Failure'],\n",
    "                yticklabels=['Actual Healthy', 'Actual Failure'])\n",
    "    plt.title('RUL Failure Detection Confusion Matrix (B0007)')\n",
    "    plt.ylabel('Actual Label')\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.show()\n",
    "\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(y_test_rul_actual, y_test_rul_pred, target_names=['Healthy', 'About to Fail'], zero_division=0))\n",
    "else:\n",
    "    print(\"\\nCould not evaluate recall as no actual failure events were found in the window.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "385ac804",
   "metadata": {},
   "source": [
    "### GRU (Gated Recurrent Unit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a213457f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import GRU\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "# --- 1. Build and Train GRU Model ---\n",
    "print('\\n--- Training GRU Model ---')\n",
    "gru_model = Sequential([\n",
    "    GRU(128, activation='tanh', return_sequences=True, input_shape=(X_train_seq.shape[1], X_train_seq.shape[2])),\n",
    "    Dropout(0.2),\n",
    "    GRU(64, activation='tanh'),\n",
    "    Dropout(0.2),\n",
    "    Dense(32, activation='relu'),\n",
    "    Dense(1)\n",
    "])\n",
    "gru_model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
    "\n",
    "history_gru = gru_model.fit(\n",
    "    X_train_seq, y_train_seq,\n",
    "    epochs=50,      # Matching the improved LSTM epochs\n",
    "    batch_size=16,\n",
    "    validation_split=0.2,\n",
    "    verbose=1 # Set to 1 to see training progress\n",
    ")\n",
    "print('GRU Training complete.')\n",
    "\n",
    "# --- 2. Evaluate GRU on Test Battery (B0007) ---\n",
    "y_pred_gru = gru_model.predict(X_test_seq).flatten()\n",
    "\n",
    "# Calculate Regression Metrics for GRU\n",
    "rmse_gru = np.sqrt(mean_squared_error(y_test_seq, y_pred_gru))\n",
    "mae_gru = mean_absolute_error(y_test_seq, y_pred_gru)\n",
    "r2_gru = r2_score(y_test_seq, y_pred_gru)\n",
    "\n",
    "# Calculate Recall for GRU (Failure Detection)\n",
    "predicted_failure_cycle_idx_gru = np.where(y_pred_gru < FAILURE_THRESHOLD)[0]\n",
    "if len(predicted_failure_cycle_idx_gru) > 0:\n",
    "    predicted_failure_cycle_gru = predicted_failure_cycle_idx_gru[0]\n",
    "    y_test_pred_gru = np.zeros_like(y_test_seq)\n",
    "    pred_warning_start_index_gru = max(0, predicted_failure_cycle_gru - RUL_WINDOW)\n",
    "    y_test_pred_gru[pred_warning_start_index_gru:predicted_failure_cycle_gru] = 1\n",
    "    recall_gru = recall_score(y_test_actual, y_test_pred_gru)\n",
    "else:\n",
    "    recall_gru = 0.0\n",
    "\n",
    "# --- 3. Get LSTM Metrics for Comparison ---\n",
    "# (Re-calculating to ensure variables are present)\n",
    "y_pred_lstm = lstm_model.predict(X_test_seq).flatten()\n",
    "rmse_lstm = np.sqrt(mean_squared_error(y_test_seq, y_pred_lstm))\n",
    "mae_lstm = mean_absolute_error(y_test_seq, y_pred_lstm)\n",
    "r2_lstm = r2_score(y_test_seq, y_pred_lstm)\n",
    "\n",
    "# Recalculate LSTM recall to be sure\n",
    "predicted_failure_cycle_idx_lstm = np.where(y_pred_lstm < FAILURE_THRESHOLD)[0]\n",
    "if len(predicted_failure_cycle_idx_lstm) > 0:\n",
    "    predicted_failure_cycle_lstm = predicted_failure_cycle_idx_lstm[0]\n",
    "    y_test_pred_lstm = np.zeros_like(y_test_seq)\n",
    "    pred_warning_start_index_lstm = max(0, predicted_failure_cycle_lstm - RUL_WINDOW)\n",
    "    y_test_pred_lstm[pred_warning_start_index_lstm:predicted_failure_cycle_lstm] = 1\n",
    "    recall_lstm = recall_score(y_test_actual, y_test_pred_lstm)\n",
    "else:\n",
    "    recall_lstm = 0.0\n",
    "\n",
    "# --- 4. Display Comparison ---\n",
    "comparison_df = pd.DataFrame({\n",
    "    'Metric': ['RMSE', 'MAE', 'R²', 'Failure Recall'],\n",
    "    'LSTM': [rmse_lstm, mae_lstm, r2_lstm, recall_lstm],\n",
    "    'GRU': [rmse_gru, mae_gru, r2_gru, recall_gru]\n",
    "})\n",
    "\n",
    "print(\"\\n--- Model Comparison (Test Battery: B0018) ---\")\n",
    "print(comparison_df)\n",
    "\n",
    "# --- 5. Visualize Predictions ---\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(y_test_seq, label='Actual SoH', color='black', linewidth=2)\n",
    "plt.plot(y_pred_lstm, label='LSTM Prediction', linestyle='--', alpha=0.8)\n",
    "plt.plot(y_pred_gru, label='GRU Prediction', linestyle='-.', alpha=0.8)\n",
    "plt.axhline(y=FAILURE_THRESHOLD, color='r', linestyle=':', label='Failure Threshold')\n",
    "plt.title('LSTM vs GRU: SoH Prediction on Test Battery B0007')\n",
    "plt.xlabel('Cycle')\n",
    "plt.ylabel('State of Health (SoH)')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "joblib.dump(gru_model, \"gru_soh_model.joblib\")\n",
    "print(\"Saved RandomForest model as gru_soh_model.joblib\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deaaab1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "comparison_df_all = pd.DataFrame({\n",
    "    'Metric': ['RMSE', 'MAE', 'R²', 'Failure Recall'],\n",
    "    'Random Forest' : [rmse_rf, mae_rf, r2_rf, 'NA' ],\n",
    "    'XGBoost' : [rmse_xgb, mae_xgb, r2_xgb, 'NA' ],\n",
    "    'LSTM': [rmse_lstm, mae_lstm, r2_lstm, recall_lstm],\n",
    "    'GRU': [rmse_gru, mae_gru, r2_gru, recall_gru]\n",
    "})\n",
    "\n",
    "print(\"\\n--- Model Comparison (Test Battery: B0018) ---\")\n",
    "print(comparison_df_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fca18100",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import GRU\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "# --- 1. Build and Train GRU Model ---\n",
    "print('\\n--- Training GRU Model ---')\n",
    "gru_model = Sequential([\n",
    "    GRU(128, activation='tanh', return_sequences=True, input_shape=(X_train_seq_rul.shape[1], X_train_seq_rul.shape[2])),\n",
    "    Dropout(0.2),\n",
    "    GRU(64, activation='tanh'),\n",
    "    Dropout(0.2),\n",
    "    Dense(32, activation='relu'),\n",
    "    Dense(1)\n",
    "])\n",
    "gru_model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
    "\n",
    "history_gru = gru_model.fit(\n",
    "    X_train_seq_rul, y_train_seq_rul,\n",
    "    epochs=50,      # Matching the improved LSTM epochs\n",
    "    batch_size=16,\n",
    "    validation_split=0.2,\n",
    "    verbose=1 # Set to 1 to see training progress\n",
    ")\n",
    "print('GRU Training complete.')\n",
    "\n",
    "# --- 2. Evaluate GRU on Test Battery (B0007) ---\n",
    "y_pred_gru = gru_model.predict(X_test_seq_rul).flatten()\n",
    "\n",
    "# Calculate Regression Metrics for GRU\n",
    "rmse_gru = np.sqrt(mean_squared_error(y_test_seq_rul, y_pred_gru))\n",
    "mae_gru = mean_absolute_error(y_test_seq_rul, y_pred_gru)\n",
    "r2_gru = r2_score(y_test_seq_rul, y_pred_gru)\n",
    "\n",
    "# Calculate Recall for GRU (Failure Detection)\n",
    "predicted_failure_cycle_idx_gru = np.where(y_pred_gru < FAILURE_THRESHOLD)[0]\n",
    "if len(predicted_failure_cycle_idx_gru) > 0:\n",
    "    predicted_failure_cycle_gru = predicted_failure_cycle_idx_gru[0]\n",
    "    y_test_rul_pred_gru = np.zeros_like(y_test_seq_rul)\n",
    "    pred_warning_start_index_gru = max(0, predicted_failure_cycle_gru - RUL_WINDOW)\n",
    "    y_test_rul_pred_gru[pred_warning_start_index_gru:predicted_failure_cycle_gru] = 1\n",
    "    recall_gru = recall_score(y_test_rul_actual, y_test_rul_pred_gru)\n",
    "else:\n",
    "    recall_gru = 0.0\n",
    "\n",
    "# --- 3. Get LSTM Metrics for Comparison ---\n",
    "# (Re-calculating to ensure variables are present)\n",
    "y_pred_lstm_rul = lstm_model_rul.predict(X_test_seq_rul).flatten()\n",
    "rmse_lstm = np.sqrt(mean_squared_error(y_test_seq_rul, y_pred_lstm_rul))\n",
    "mae_lstm = mean_absolute_error(y_test_seq_rul, y_pred_lstm_rul)\n",
    "r2_lstm = r2_score(y_test_seq_rul, y_pred_lstm_rul)\n",
    "\n",
    "# Recalculate LSTM recall to be sure\n",
    "predicted_failure_cycle_idx_lstm = np.where(y_pred_lstm_rul < FAILURE_THRESHOLD)[0]\n",
    "if len(predicted_failure_cycle_idx_lstm) > 0:\n",
    "    predicted_failure_cycle_lstm = predicted_failure_cycle_idx_lstm[0]\n",
    "    y_test_rul_pred_lstm = np.zeros_like(y_test_seq_rul)\n",
    "    pred_warning_start_index_lstm = max(0, predicted_failure_cycle_lstm - RUL_WINDOW)\n",
    "    y_test_rul_pred_lstm[pred_warning_start_index_lstm:predicted_failure_cycle_lstm] = 1\n",
    "    recall_lstm = recall_score(y_test_rul_actual, y_test_rul_pred_lstm)\n",
    "else:\n",
    "    recall_lstm = 0.0\n",
    "\n",
    "# --- 4. Display Comparison ---\n",
    "comparison_df = pd.DataFrame({\n",
    "    'Metric': ['RMSE', 'MAE', 'R²', 'Failure Recall'],\n",
    "    'LSTM': [rmse_lstm, mae_lstm, r2_lstm, recall_lstm],\n",
    "    'GRU': [rmse_gru, mae_gru, r2_gru, recall_gru]\n",
    "})\n",
    "\n",
    "print(\"\\n--- Model Comparison (Test Battery: B0007) ---\")\n",
    "print(comparison_df)\n",
    "\n",
    "# --- 5. Visualize Predictions ---\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(y_test_seq_rul, label='Actual SoH', color='black', linewidth=2)\n",
    "plt.plot(y_pred_lstm_rul, label='LSTM Prediction', linestyle='--', alpha=0.8)\n",
    "plt.plot(y_pred_gru, label='GRU Prediction', linestyle='-.', alpha=0.8)\n",
    "plt.axhline(y=FAILURE_THRESHOLD, color='r', linestyle=':', label='Failure Threshold')\n",
    "plt.title('LSTM vs GRU: SoH Prediction on Test Battery B0007')\n",
    "plt.xlabel('Cycle')\n",
    "plt.ylabel('State of Health (SoH)')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4cef281",
   "metadata": {},
   "source": [
    "Detailed performance analysis: EV battery health prediction models\n",
    "\n",
    "Executive summary\n",
    "\n",
    "The results show clear differences across models and test batteries. GRU is the most consistent deep learning model, while Random Forest leads on B0018. LSTM struggles on B0007, indicating sensitivity to battery-specific patterns.\n",
    "\n",
    "---\n",
    "\n",
    "Part 1: Test battery B0007 (challenging case)\n",
    "\n",
    "| Metric | LSTM | GRU | Winner |\n",
    "|--------|------|-----|--------|\n",
    "| RMSE | 0.0582 | 0.0315 | GRU (46% lower) |\n",
    "| MAE | 0.0573 | 0.0290 | GRU (49% lower) |\n",
    "| R² | 0.2940 | 0.7927 | GRU (2.7x higher) |\n",
    "| Failure Recall | 0.00% | 0.00% | N/A (no failures) |\n",
    "\n",
    "Findings\n",
    "\n",
    "1. GRU significantly outperforms LSTM\n",
    "   - R²: 0.79 vs 0.29\n",
    "   - MAE: 2.90% vs 5.73%\n",
    "   - GRU captures the degradation pattern; LSTM does not\n",
    "\n",
    "2. LSTM struggles on B0007\n",
    "   - R² = 0.29 indicates poor fit\n",
    "   - Likely due to different degradation characteristics or insufficient training data for this battery\n",
    "\n",
    "3. Training dynamics\n",
    "   - GRU: loss drops from 0.1362 to 0.0023; validation MAE from 0.0342 to 0.0734 (some overfitting)\n",
    "   - LSTM: similar training pattern but poor generalization on B0007\n",
    "\n",
    "---\n",
    "\n",
    "Part 2: Test battery B0018 (healthy battery)\n",
    "\n",
    "| Metric | Random Forest | XGBoost | LSTM | GRU | Winner |\n",
    "|--------|---------------|---------|------|-----|--------|\n",
    "| RMSE | 0.0206 | 0.0220 | 0.0320 | 0.0212 | Random Forest |\n",
    "| MAE | 0.0170 | 0.0176 | 0.0283 | 0.0150 | GRU |\n",
    "| R² | 0.9367 | 0.9276 | 0.8096 | 0.9166 | Random Forest |\n",
    "| Failure Recall | N/A | N/A | 0.00% | 0.00% | N/A |\n",
    "\n",
    "Findings\n",
    "\n",
    "1. Random Forest leads overall\n",
    "   - Highest R² (0.9367)\n",
    "   - Lowest RMSE (0.0206)\n",
    "   - Strong, interpretable baseline\n",
    "\n",
    "2. GRU has the lowest MAE\n",
    "   - MAE: 1.50% (best)\n",
    "   - R²: 0.9166 (strong)\n",
    "   - Best deep learning option for point accuracy\n",
    "\n",
    "3. XGBoost is competitive\n",
    "   - R²: 0.9276\n",
    "   - MAE: 1.76%\n",
    "   - Close to Random Forest\n",
    "\n",
    "4. LSTM underperforms\n",
    "   - R²: 0.8096 (lowest)\n",
    "   - MAE: 2.83% (highest)\n",
    "   - Still acceptable, but weaker than alternatives\n",
    "\n",
    "---\n",
    "\n",
    "Part 3: Cross-battery performance comparison\n",
    "\n",
    "#### Model consistency\n",
    "\n",
    "1. GRU: most consistent\n",
    "   - B0007: R² = 0.79, MAE = 2.90%\n",
    "   - B0018: R² = 0.92, MAE = 1.50%\n",
    "   - Robust across different battery behaviors\n",
    "\n",
    "2. LSTM: inconsistent\n",
    "   - B0007: R² = 0.29 (poor)\n",
    "   - B0018: R² = 0.81 (acceptable)\n",
    "   - High variance suggests sensitivity to data characteristics\n",
    "\n",
    "3. Random Forest & XGBoost: consistent (B0018 only)\n",
    "   - Both achieve R² > 0.92\n",
    "   - Not tested on B0007 in this comparison\n",
    "\n",
    "---\n",
    "\n",
    "Part 4: Training analysis\n",
    "\n",
    "GRU training (50 epochs)\n",
    "- Initial loss: 0.1362 → Final loss: 0.0023 (98% reduction)\n",
    "- Validation loss: 0.0018 → 0.0058 (some overfitting)\n",
    "- Training MAE: 0.2767 → 0.0347 (87% reduction)\n",
    "- Conclusion: strong learning, with slight overfitting\n",
    "\n",
    "LSTM training (50 epochs for B0018, 100 epochs for improved architecture)\n",
    "- B0018: loss 0.5206 → 0.0047 (99% reduction)\n",
    "- Improved architecture: loss 0.1257 → 0.0014 (99% reduction)\n",
    "- Validation loss remains low, indicating good generalization on B0018\n",
    "- Conclusion: trains well but generalizes poorly on B0007\n",
    "\n",
    "---\n",
    "\n",
    "Part 5: Failure detection (RUL prediction)\n",
    "\n",
    "- All models: 0% recall\n",
    "- Reason: B0007 and B0018 did not fail below the 70% threshold in the test windows\n",
    "- This is expected and not a model failure\n",
    "\n",
    "---\n",
    "\n",
    "Part 6: Model ranking and recommendations\n",
    "\n",
    "Overall ranking (SoH prediction)\n",
    "\n",
    "1. GRU\n",
    "   - Best MAE on B0018 (1.50%)\n",
    "   - Strong R² (0.79–0.92)\n",
    "   - Most consistent across batteries\n",
    "   - Best deep learning choice\n",
    "\n",
    "2. Random Forest\n",
    "   - Highest R² on B0018 (0.9367)\n",
    "   - Lowest RMSE (0.0206)\n",
    "   - Interpretable and reliable\n",
    "   - Best traditional ML choice\n",
    "\n",
    "3. XGBoost\n",
    "   - R²: 0.9276, MAE: 1.76%\n",
    "   - Very close to Random Forest\n",
    "   - Strong alternative\n",
    "\n",
    "4. LSTM\n",
    "   - Inconsistent (R² 0.29–0.81)\n",
    "   - Higher error rates\n",
    "   - Needs architecture tuning or more data\n",
    "\n",
    "---\n",
    "\n",
    "Part 7: Project goal achievement\n",
    "\n",
    "| Goal | Target | Achievement | Status |\n",
    "|------|--------|-------------|--------|\n",
    "| SoH prediction accuracy | ±5% error | GRU: 1.50% MAE | Exceeded |\n",
    "| RUL failure detection | >90% recall | Cannot evaluate (no failures) | N/A |\n",
    "| Model deployment | Scalable pipeline | All models deployable | Achieved |\n",
    "\n",
    "---\n",
    "\n",
    "Part 8: Final recommendations\n",
    "\n",
    "1. Primary model: GRU\n",
    "   - Best balance of accuracy and consistency\n",
    "   - Suitable for production deployment\n",
    "\n",
    "2. Baseline model: Random Forest\n",
    "   - Highest R², interpretable\n",
    "   - Good for comparison and explainability\n",
    "\n",
    "3. LSTM improvements\n",
    "   - Tune architecture (layers, units, dropout)\n",
    "   - Add regularization\n",
    "   - Consider more training data or data augmentation\n",
    "\n",
    "4. Next steps\n",
    "   - Test on batteries that actually fail to evaluate RUL recall\n",
    "   - Ensemble GRU + Random Forest for robustness\n",
    "   - Hyperparameter tuning for all models\n",
    "\n",
    "---\n",
    "\n",
    "Conclusion\n",
    "\n",
    "The GRU model is the top performer for SoH prediction, with the lowest MAE and strong consistency. Random Forest provides the best overall fit (highest R²) and interpretability. Both meet the ±5% error target. LSTM requires further optimization to match the others.\n",
    "\n",
    "**LSTMs** typically have three \"gates\" (input, forget, and output gates) and a separate \"cell state\" to control the flow of information. This makes them quite powerful for capturing very long-term dependencies.\n",
    "\n",
    "**GRUs** are a simplified version of LSTMs. They have only two \"gates\" (reset and update gates) and combine the cell state and hidden state. \n",
    "The update gate in a GRU essentially combines the functionality of the forget and input gates in an LSTM. This can make GRUs more efficient at capturing relevant information and discarding irrelevant information for certain types of sequences\n",
    "\n",
    "This simpler structure means GRUs have fewer parameters to learn compared to LSTMs.\n",
    "**Reduced Overfitting:** With fewer parameters, GRUs are generally less prone to overfitting, especially on datasets that are not extremely large or complex. In the context of battery degradation, while sequential, the underlying degradation patterns might not require the full complexity of an LSTM's memory mechanisms.\n",
    "**Faster Training:** Fewer parameters also translate to faster training times, which can be an advantage during model development and iteration."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fecef3c",
   "metadata": {},
   "source": [
    "### Unsupervised Learning with K-Means Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aa8a0dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# --- 1. Prepare Data for Clustering ---\n",
    "# Using scaled training and testing data to identify patterns across all batteries.\n",
    "X_clustering = pd.concat([X_train_scaled_df, X_test_scaled_df])\n",
    "\n",
    "print(f\"Data shape for clustering: {X_clustering.shape}\")\n",
    "\n",
    "# --- 2. Determine Optimal K (Elbow Method & Silhouette Score) ---\n",
    "inertias = []\n",
    "silhouette_scores = []\n",
    "K_range = range(2, 10)  # Test between 2 and 9 clusters\n",
    "\n",
    "print(\"Calculating optimal K...\")\n",
    "for k in K_range:\n",
    "    # n_init='auto' suppresses a future warning in newer sklearn versions\n",
    "    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10) \n",
    "    kmeans.fit(X_clustering)\n",
    "    inertias.append(kmeans.inertia_)\n",
    "    silhouette_scores.append(silhouette_score(X_clustering, kmeans.labels_))\n",
    "\n",
    "# --- 3. Plot Elbow Curve and Silhouette Scores ---\n",
    "fig, ax1 = plt.subplots(figsize=(12, 5))\n",
    "\n",
    "# Plot Inertia (Elbow Curve)\n",
    "color = 'tab:blue'\n",
    "ax1.set_xlabel('Number of Clusters (k)')\n",
    "ax1.set_ylabel('Inertia (Sum of Squared Distances)', color=color)\n",
    "ax1.plot(K_range, inertias, 'bx-', label='Inertia')\n",
    "ax1.tick_params(axis='y', labelcolor=color)\n",
    "ax1.set_title('Elbow Method & Silhouette Score for Optimal k')\n",
    "ax1.grid(True)\n",
    "\n",
    "# Plot Silhouette Score on secondary y-axis\n",
    "ax2 = ax1.twinx()\n",
    "color = 'tab:red'\n",
    "ax2.set_ylabel('Silhouette Score', color=color)\n",
    "ax2.plot(K_range, silhouette_scores, 'rx-', label='Silhouette Score')\n",
    "ax2.tick_params(axis='y', labelcolor=color)\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# --- 4. Apply K-Means with Optimal K ---\n",
    "# We select the K with the highest Silhouette Score as it indicates the best-defined clusters.\n",
    "optimal_k = K_range[np.argmax(silhouette_scores)]\n",
    "print(f\"Optimal K based on Silhouette Score: {optimal_k}\")\n",
    "\n",
    "kmeans_final = KMeans(n_clusters=optimal_k, random_state=42, n_init=10)\n",
    "clusters = kmeans_final.fit_predict(X_clustering)\n",
    "\n",
    "# --- 5. Analyze Cluster Characteristics ---\n",
    "# We add the cluster labels back to a copy of the original dataframe to interpret them.\n",
    "clustered_df = final_df.copy()\n",
    "# Ensure we align by index since X_clustering was a concatenation\n",
    "clustered_df.loc[X_clustering.index, 'cluster'] = clusters\n",
    "\n",
    "# Group by cluster and calculate the mean of features to define the \"profile\" of each cluster.\n",
    "# We include 'SoH' to see how health correlates with these unsupervised clusters.\n",
    "cluster_summary = clustered_df.groupby('cluster')[features + ['SoH']].mean()\n",
    "print(\"\\n--- Cluster Characteristics (Mean Values) ---\")\n",
    "print(cluster_summary)\n",
    "\n",
    "# Count of samples in each cluster\n",
    "print(\"\\n--- Cluster Sizes ---\")\n",
    "print(clustered_df['cluster'].value_counts())\n",
    "\n",
    "# --- 6. Visualize Clusters ---\n",
    "# Scatter plot of Cycle vs. SoH, colored by Cluster\n",
    "# This visualizes how the algorithm has segmented the battery life.\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.scatterplot(data=clustered_df, x='cycle', y='SoH', hue='cluster', palette='viridis', style='battery_name')\n",
    "plt.title(f'K-Means Clustering (k={optimal_k}): SoH Degradation Stages')\n",
    "plt.xlabel('Cycle Number')\n",
    "plt.ylabel('State of Health (SoH)')\n",
    "#plt.legend(title='Cluster')\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
